{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xgmNpu_tTee",
        "outputId": "24f78873-f43f-41b6-c937-1bf931bdc8ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: earthengine-api in /usr/local/lib/python3.11/dist-packages (1.5.7)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from earthengine-api) (2.19.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.12.1 in /usr/local/lib/python3.11/dist-packages (from earthengine-api) (2.164.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from earthengine-api) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from earthengine-api) (0.2.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from earthengine-api) (0.22.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from earthengine-api) (2.32.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.1->earthengine-api) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.12.1->earthengine-api) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.4.1->earthengine-api) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.4.1->earthengine-api) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.4.1->earthengine-api) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1dev,>=0.9.2->earthengine-api) (3.2.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->earthengine-api) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->earthengine-api) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->earthengine-api) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->earthengine-api) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->earthengine-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->earthengine-api) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->earthengine-api) (2025.1.31)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (5.29.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.26.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api) (0.6.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install earthengine-api\n",
        "!pip install tensorflow opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "OC4_te-Jujjh",
        "outputId": "4f913244-faeb-41e1-9d8b-5f5e1894c427"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-6bc5b35fbb9b>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6bc5b35fbb9b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    earthengine authenticate\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "earthengine authenticate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da_dQvu8u_Lo"
      },
      "outputs": [],
      "source": [
        "!pip install earthengine-api\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhT6kwRtuobN"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQPmrZ1hvFo7"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa--f_zOvTgt"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='earthengine-legacy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5OufRiowG8c"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='YOUR_PROJECT_ID')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyaoF_-xwb1K"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ai-marine-cleanup')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQPeVZeG740k"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJP1pTRC78m_"
      },
      "outputs": [],
      "source": [
        "ee.Initialize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fALlEt5O7tGk"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojJ8-fgq8I7Z"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project=ai-marine-cleanup)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn5lZUW58fI_"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ai-marine-cleanup')  # Project ID quotes me hona chahiye\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_LEsIDs84KL"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuZf9Hrh86vq"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()  # Authenticate kro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUXSatA489Qu"
      },
      "outputs": [],
      "source": [
        "ee.Initialize(project='ai-marine-cleanup')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiMHBxHt9msJ"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqxgTXVM9qGD"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import folium\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGEjJJQQ997s"
      },
      "outputs": [],
      "source": [
        "# Load Sentinel-2 image (replace with your actual image)\n",
        "image = ee.Image('COPERNICUS/S2/20220101T000239_20220101T000824_T56JLP')\n",
        "\n",
        "# Visualize the image details\n",
        "print(image.getInfo())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8WKHRshCf5w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5WUDwXwCiiC"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Check OpenCV version\n",
        "print(\"OpenCV version:\", cv2.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogUi-k1yClYp"
      },
      "outputs": [],
      "source": [
        "# Simple CNN Model for image classification (test run)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')  # Change number of classes as needed\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRlah3VUCoIL"
      },
      "outputs": [],
      "source": [
        "# Test OpenCV with an image (replace with your image path)\n",
        "image_path = 'path_to_an_image.jpg'  # Add a valid image path\n",
        "image = cv2.imread(image_path)\n",
        "if image is not None:\n",
        "    print(\"Image loaded successfully!\")\n",
        "else:\n",
        "    print(\"Failed to load image.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1UquMVAKuiP"
      },
      "outputs": [],
      "source": [
        "# Install and authenticate Google Earth Engine\n",
        "!pip install earthengine-api\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_ytWzA9LMuR"
      },
      "outputs": [],
      "source": [
        "!pip install earthengine-api\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ai-marine-cleanup')  # Replace 'your-project-id' with your actual project ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKhly-RYLXag"
      },
      "outputs": [],
      "source": [
        "# Define an ocean region (e.g., part of the Pacific Ocean)\n",
        "region = ee.Geometry.Rectangle([-160, -20, -150, -10])\n",
        "\n",
        "# Get Sentinel-2 images (high-res satellite data)\n",
        "dataset = ee.ImageCollection('COPERNICUS/S2') \\\n",
        "           .filterDate('2024-01-01', '2024-12-31') \\\n",
        "           .filterBounds(region) \\\n",
        "           .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
        "\n",
        "# Select relevant bands (e.g., RGB)\n",
        "ocean_images = dataset.select(['B4', 'B3', 'B2'])\n",
        "\n",
        "print(ocean_images.size().getInfo())  # To check number of images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvnZq1kfLedO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Export image to Google Drive\n",
        "image = ocean_images.first()\n",
        "task = ee.batch.Export.image.toDrive(**{\n",
        "    'image': image,\n",
        "    'description': 'ocean_image_export',\n",
        "    'scale': 30,\n",
        "    'region': region,\n",
        "    'fileFormat': 'GeoTIFF'\n",
        "})\n",
        "task.start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCNpwJ4NLsYi"
      },
      "outputs": [],
      "source": [
        "!pip install labelImg\n",
        "!labelImg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZRIKuZgL3Xm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # For binary classification (waste/no-waste)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFedzl5SMH7-"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='your-project-id')  # Replace with your project ID\n",
        "\n",
        "# Load a satellite image collection\n",
        "dataset = ee.ImageCollection('COPERNICUS/S2') \\\n",
        "             .filterDate('2024-01-01', '2024-02-01') \\\n",
        "             .filterBounds(ee.Geometry.Point([78.4867, 17.3850]))  # Example: Hyderabad coordinates\n",
        "\n",
        "# Visualize image metadata\n",
        "images = dataset.toList(5)\n",
        "for i in range(5):\n",
        "    image = ee.Image(images.get(i))\n",
        "    print(image.getInfo())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7ySVALUNU4P"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ai-marine-cleanup')  # Replace with your actual project ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "disYxCmTOJyN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "392y_NCdOYn7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,         # Rotate images by 20 degrees\n",
        "    width_shift_range=0.2,     # Shift width by 20%\n",
        "    height_shift_range=0.2,    # Shift height by 20%\n",
        "    shear_range=0.2,           # Shear transformations\n",
        "    zoom_range=0.2,            # Random zoom\n",
        "    horizontal_flip=True,      # Random horizontal flip\n",
        "    fill_mode='nearest'        # Fill in missing pixels after transformation\n",
        ")\n",
        "\n",
        "# Apply data augmentation to your training dataset\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    'path_to_training_data',    # Path to your training images\n",
        "    target_size=(224, 224),     # Resize all images to 224x224 (adjust if needed)\n",
        "    batch_size=32,              # Number of images in each batch\n",
        "    class_mode='binary'         # or 'categorical' depending on your labels\n",
        ")\n",
        "\n",
        "print(\"Data augmentation applied successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPcq0I-vSwUE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Initialize data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load training data\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    r'C:\\Users\\dell\\Downloads\\archive (1)\\train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load validation data\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    r'C:\\Users\\dell\\Downloads\\archive (1)\\val',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load test data\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    r'C:\\Users\\dell\\Downloads\\archive (1)\\test',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgmzNawUTAkk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFhHOB0HTJ_v"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/drive/My Drive/Downloads/archive (1)/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j4vEj9wTRBi"
      },
      "outputs": [],
      "source": [
        "/content/drive/My Drive/archive (1)/train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiYfBDQRTkvQ"
      },
      "outputs": [],
      "source": [
        "!unzip /content/archive\\ \\(1\\).zip -d /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46JGT6DXTz-T"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnB6Lt7bT5_6"
      },
      "outputs": [],
      "source": [
        "!unzip '/content/drive/My Drive/archive (1).zip' -d '/content/drive/My Drive/dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrdJUFtXVPiS"
      },
      "outputs": [],
      "source": [
        "val_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/val',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/test',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7SxBLgIVcEs"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_generator)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynv6kMhLVjNF"
      },
      "outputs": [],
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/train',   # Ensure this path is correct\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNit2Ay7Vn40"
      },
      "outputs": [],
      "source": [
        "!ls /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_r5AYpzVwPo"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlKcuk6XkuDd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5P_P0k2k9zT"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/path_to/archive\\ \\(1\\).zip /content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpmE8TV6lCrS"
      },
      "outputs": [],
      "source": [
        "!unzip /content/archive\\ \\(1\\).zip -d /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdoZdqXclTy_"
      },
      "outputs": [],
      "source": [
        "!ls /content/dataset\n",
        "!ls /content/dataset/train\n",
        "!ls /content/dataset/val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5Fd9hyslYVU"
      },
      "outputs": [],
      "source": [
        "/content/dataset\n",
        "├── train\n",
        "│   ├── class_1\n",
        "│   ├── class_2\n",
        "│   └── ...\n",
        "├── val\n",
        "│   ├── class_1\n",
        "│   ├── class_2\n",
        "│   └── ...\n",
        "└── test\n",
        "    ├── class_1\n",
        "    ├── class_2\n",
        "    └── ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39RlNOHglk7n"
      },
      "outputs": [],
      "source": [
        "!ls /content/dataset\n",
        "!ls /content/dataset/train\n",
        "# etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqydtMX3msbQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCCZ-qy3mvNd"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive/Downloads\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Qc_-z6mmxEI"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive/Downloads\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMfKY51zmzQ9"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Downloads/archive (1).zip\" -d \"/content/dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6OCnbUdoC7g"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CUAlIlHoFr4"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive/Downloads\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_p8Jzoepe_f"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/archive\\ (1).zip\" -d \"/content/dataset\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-gkPUZ1pqmG"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZH7jtZipwi1"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/archive (1).zip\" -d /content/dataset/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkjAxWkqp3vY"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set up data generators for training and validation sets\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/train',   # Ensure this path is correct after extraction\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/val',   # Ensure this path is correct after extraction\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3u0v5F1p_KW"
      },
      "outputs": [],
      "source": [
        "!pip install keras tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sphf7B8jqBbX"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Set up data generators for training and validation sets\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/train',   # Ensure this path is correct after extraction\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    '/content/dataset/val',   # Ensure this path is correct after extraction\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVIHKRUfqCf9"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y keras tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IAbSRIdqLtg"
      },
      "outputs": [],
      "source": [
        "!pip install keras==2.15.0 tensorflow==2.15.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeJJxLVuqfzt"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOuG-v7_q5Sg"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path/to/train/data',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Or 'categorical' for multi-class classification\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'path/to/validation/data',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Or 'categorical' for multi-class classification\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osRd6fJDrG6Y"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/dataset/train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Use 'categorical' if it's multi-class\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/content/dataset/val',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Use 'categorical' if it's multi-class\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1MH4UMerSmc"
      },
      "outputs": [],
      "source": [
        "/content/dataset/train/class_1\n",
        "/content/dataset/train/class_2\n",
        "/content/dataset/val/class_1\n",
        "/content/dataset/val/class_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OulJILORrYGv"
      },
      "outputs": [],
      "source": [
        "/content/dataset/train/class_1\n",
        "/content/dataset/train/class_2\n",
        "/content/dataset/val/class_1\n",
        "/content/dataset/val/class_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8WXMa0Sralm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('/content/dataset/train'))  # List files inside the train directory\n",
        "print(os.listdir('/content/dataset/val'))    # List files inside the val directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJAhX27Nr2wh"
      },
      "outputs": [],
      "source": [
        "/content/dataset/train/class_1\n",
        "/content/dataset/train/class_2\n",
        "/content/dataset/val/class_1\n",
        "/content/dataset/val/class_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXZdAXS-r5tW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('/content/dataset/train'))  # Check train folder\n",
        "print(os.listdir('/content/dataset/val'))    # Check val folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBh-_fvGr8AM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_directory = '/content/dataset/train'  # Ensure this path is correct\n",
        "val_directory = '/content/dataset/val'      # Ensure this path is correct\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_directory,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # or 'categorical' depending on your setup\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_directory,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # or 'categorical' depending on your setup\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOd2gNlksCvE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check that the train and val directories exist and contain the expected folders\n",
        "print(\"Train Directory Contents:\", os.listdir('/content/dataset/train'))\n",
        "print(\"Validation Directory Contents:\", os.listdir('/content/dataset/val'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g50eEqqksFEP"
      },
      "outputs": [],
      "source": [
        "/content/dataset/train/class_1/\n",
        "/content/dataset/train/class_2/\n",
        "/content/dataset/val/class_1/\n",
        "/content/dataset/val/class_2/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmwmWpfCsMsa"
      },
      "outputs": [],
      "source": [
        "/content/dataset/train/class_1/\n",
        "/content/dataset/train/class_2/\n",
        "/content/dataset/val/class_1/\n",
        "/content/dataset/val/class_2/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFi-dmgnsUIT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Verify that the directory structure is correct\n",
        "print(\"Train Directory:\", os.listdir('/content/dataset/train'))\n",
        "print(\"Validation Directory:\", os.listdir('/content/dataset/val'))\n",
        "\n",
        "# Check the subdirectories for the classes\n",
        "print(\"Class 1 Train Images:\", os.listdir('/content/dataset/train/class_1'))\n",
        "print(\"Class 2 Train Images:\", os.listdir('/content/dataset/train/class_2'))\n",
        "print(\"Class 1 Val Images:\", os.listdir('/content/dataset/val/class_1'))\n",
        "print(\"Class 2 Val Images:\", os.listdir('/content/dataset/val/class_2'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8LHpbZ3shme"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "\n",
        "# Create image generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Function to filter out non-image files\n",
        "def filter_images(directory):\n",
        "    return [f for f in os.listdir(directory) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# Use the filtered lists for loading data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    classes=['class_1', 'class_2'],  # Make sure to list your classes here\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    classes=['class_1', 'class_2'],  # Make sure to list your classes here\n",
        "    subset='validation'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7gWSHq8sj1o"
      },
      "outputs": [],
      "source": [
        "print(\"Train class_1 images:\", os.listdir('/content/dataset/train/class_1'))\n",
        "print(\"Train class_2 images:\", os.listdir('/content/dataset/train/class_2'))\n",
        "print(\"Val class_1 images:\", os.listdir('/content/dataset/val/class_1'))\n",
        "print(\"Val class_2 images:\", os.listdir('/content/dataset/val/class_2'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1C4PQZUssyP"
      },
      "outputs": [],
      "source": [
        "# Check the directory structure\n",
        "print(\"Train class_1 images:\", os.listdir('/content/dataset/train/class_1') if os.path.exists('/content/dataset/train/class_1') else \"Directory not found\")\n",
        "print(\"Train class_2 images:\", os.listdir('/content/dataset/train/class_2') if os.path.exists('/content/dataset/train/class_2') else \"Directory not found\")\n",
        "print(\"Val class_1 images:\", os.listdir('/content/dataset/val/class_1') if os.path.exists('/content/dataset/val/class_1') else \"Directory not found\")\n",
        "print(\"Val class_2 images:\", os.listdir('/content/dataset/val/class_2') if os.path.exists('/content/dataset/val/class_2') else \"Directory not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1U4GQWgszgE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create directory structure if it doesn't exist\n",
        "os.makedirs('/content/dataset/train/class_1', exist_ok=True)\n",
        "os.makedirs('/content/dataset/train/class_2', exist_ok=True)\n",
        "os.makedirs('/content/dataset/val/class_1', exist_ok=True)\n",
        "os.makedirs('/content/dataset/val/class_2', exist_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWCGPZyhs1zR"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Example of moving files\n",
        "shutil.move('path_to_your_image/image1.jpg', '/content/dataset/train/class_1/image1.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5PgGEDmtAiA"
      },
      "outputs": [],
      "source": [
        "shutil.move('/content/dataset/source_folder/image1.jpg', '/content/dataset/train/class_1/image1.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utGNH0E5tKgB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "source_folder = '/content/dataset/source_folder/'\n",
        "print(os.listdir(source_folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n6TJdLItP7F"
      },
      "outputs": [],
      "source": [
        "print(os.listdir('/content/dataset'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agN0XJJctPzZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the paths for each directory\n",
        "base_dir = '/content/dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'val')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Create subfolders for each class (class_1, class_2)\n",
        "os.makedirs(os.path.join(train_dir, 'class_1'), exist_ok=True)\n",
        "os.makedirs(os.path.join(train_dir, 'class_2'), exist_ok=True)\n",
        "\n",
        "os.makedirs(os.path.join(val_dir, 'class_1'), exist_ok=True)\n",
        "os.makedirs(os.path.join(val_dir, 'class_2'), exist_ok=True)\n",
        "\n",
        "os.makedirs(os.path.join(test_dir, 'class_1'), exist_ok=True)\n",
        "os.makedirs(os.path.join(test_dir, 'class_2'), exist_ok=True)\n",
        "\n",
        "print(\"Subdirectories created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-uXMl0etpg8"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Define the source directory where your images are located\n",
        "source_folder = '/content/dataset/source_folder/'\n",
        "\n",
        "# Define the target directories for class_1 and class_2\n",
        "class_1_train = os.path.join(train_dir, 'class_1')\n",
        "class_2_train = os.path.join(train_dir, 'class_2')\n",
        "\n",
        "# Move or copy images from source folder to class folders\n",
        "for img in os.listdir(source_folder):\n",
        "    # For class_1\n",
        "    if 'class_1' in img:  # You can adjust this condition based on your image names\n",
        "        shutil.move(os.path.join(source_folder, img), os.path.join(class_1_train, img))\n",
        "    # For class_2\n",
        "    elif 'class_2' in img:\n",
        "        shutil.move(os.path.join(source_folder, img), os.path.join(class_2_train, img))\n",
        "\n",
        "print(\"Images moved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36vhD-Kpty0h"
      },
      "outputs": [],
      "source": [
        "source_folder = '/content/dataset/images/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Piku1Twt0vb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "source_folder = '/content/dataset/images/'  # Replace with the correct path\n",
        "print(os.listdir(source_folder))  # Check the contents of the source folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOEPj_Wkt0ox"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('/content/dataset/'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgoKdUkNt0hW"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FXAcTfwuJH8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOVtESC5uUn8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUqQsagNuYRc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # This will allow you to select all images at once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLUNNCzOulL8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJTE9dh3uta-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "images_folder = '/content/drive/MyDrive/your_folder_name/'  # Replace with the correct folder path\n",
        "print(os.listdir(images_folder))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ba22C7Xu1pw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "drive_path = '/content/drive/MyDrive/'\n",
        "print(os.listdir(drive_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Q4yuGju9Ck"
      },
      "outputs": [],
      "source": [
        "images_folder = '/content/drive/MyDrive/dataset/'\n",
        "print(os.listdir(images_folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2GodK9uvCid"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "train_folder = '/content/drive/MyDrive/dataset/train/'\n",
        "class_1_folder = os.path.join(train_folder, 'class_1')\n",
        "class_2_folder = os.path.join(train_folder, 'class_2')\n",
        "\n",
        "# Ensure class folders exist\n",
        "os.makedirs(class_1_folder, exist_ok=True)\n",
        "os.makedirs(class_2_folder, exist_ok=True)\n",
        "\n",
        "# Source folder where images are located (you can change this to 'test' or 'val' depending on where you want to source from)\n",
        "source_folder = '/content/drive/MyDrive/dataset/train/'  # Or 'test' or 'val'\n",
        "\n",
        "# Loop through the images and move them to the appropriate class folders\n",
        "for img in os.listdir(source_folder):\n",
        "    img_path = os.path.join(source_folder, img)\n",
        "    if 'class_1' in img:  # Adjust condition based on your image names\n",
        "        shutil.move(img_path, os.path.join(class_1_folder, img))\n",
        "    elif 'class_2' in img:  # Adjust condition based on your image names\n",
        "        shutil.move(img_path, os.path.join(class_2_folder, img))\n",
        "\n",
        "print(\"Images have been moved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORu5rbIEvKvd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define paths\n",
        "train_folder = '/content/drive/MyDrive/dataset/train/'\n",
        "class_1_folder = os.path.join(train_folder, 'class_1')\n",
        "class_2_folder = os.path.join(train_folder, 'class_2')\n",
        "\n",
        "# Ensure class folders exist\n",
        "os.makedirs(class_1_folder, exist_ok=True)\n",
        "os.makedirs(class_2_folder, exist_ok=True)\n",
        "\n",
        "# Source folder where images are located (you can change this to 'test' or 'val' depending on where you want to source from)\n",
        "source_folder = '/content/drive/MyDrive/dataset/train/'  # Or 'test' or 'val'\n",
        "\n",
        "# Loop through the images and move them to the appropriate class folders\n",
        "for img in os.listdir(source_folder):\n",
        "    img_path = os.path.join(source_folder, img)\n",
        "\n",
        "    # Check if it's a file, not a folder\n",
        "    if os.path.isfile(img_path):\n",
        "        if 'class_1' in img:  # Adjust condition based on your image names\n",
        "            shutil.move(img_path, os.path.join(class_1_folder, img))\n",
        "        elif 'class_2' in img:  # Adjust condition based on your image names\n",
        "            shutil.move(img_path, os.path.join(class_2_folder, img))\n",
        "\n",
        "print(\"Images have been moved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX9EuFhqvZVH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "dataset_folder = '/content/drive/MyDrive/dataset'\n",
        "print(os.listdir(dataset_folder))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGZt9xtMvj5R"
      },
      "outputs": [],
      "source": [
        "print(os.listdir('/content/drive/MyDrive/dataset/train'))\n",
        "print(os.listdir('/content/drive/MyDrive/dataset/val'))\n",
        "print(os.listdir('/content/drive/MyDrive/dataset/test'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NrPF8Ntvn2U"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# ImageDataGenerator to rescale images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow from directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset/train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # or 'categorical' if you have more classes\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset/val',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # or 'categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset/test',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # or 'categorical'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usqNlhR7vw7l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check contents of train, val, and test folders\n",
        "print(\"Train folder:\", os.listdir('/content/drive/MyDrive/dataset/train'))\n",
        "print(\"Val folder:\", os.listdir('/content/drive/MyDrive/dataset/val'))\n",
        "print(\"Test folder:\", os.listdir('/content/drive/MyDrive/dataset/test'))\n",
        "\n",
        "# Check class folders\n",
        "print(\"Train class_1:\", os.listdir('/content/drive/MyDrive/dataset/train/class_1'))\n",
        "print(\"Train class_2:\", os.listdir('/content/drive/MyDrive/dataset/train/class_2'))\n",
        "print(\"Val class_1:\", os.listdir('/content/drive/MyDrive/dataset/val/class_1'))\n",
        "print(\"Val class_2:\", os.listdir('/content/drive/MyDrive/dataset/val/class_2'))\n",
        "print(\"Test class_1:\", os.listdir('/content/drive/MyDrive/dataset/test/class_1'))\n",
        "print(\"Test class_2:\", os.listdir('/content/drive/MyDrive/dataset/test/class_2'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUWQnhBIv8d4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path for the class folders inside 'train', 'val', and 'test'\n",
        "class_folders = [\n",
        "    '/content/drive/MyDrive/dataset/train/class_1',\n",
        "    '/content/drive/MyDrive/dataset/train/class_2',\n",
        "    '/content/drive/MyDrive/dataset/val/class_1',\n",
        "    '/content/drive/MyDrive/dataset/val/class_2',\n",
        "    '/content/drive/MyDrive/dataset/test/class_1',\n",
        "    '/content/drive/MyDrive/dataset/test/class_2'\n",
        "]\n",
        "\n",
        "# Create the directories if they do not exist\n",
        "for folder in class_folders:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "print(\"Directories ensured to exist!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExiBiF42wDXg"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source folder containing the images\n",
        "source_folder = '/content/drive/MyDrive/dataset/images/'  # Update with your source folder path\n",
        "\n",
        "# Define destination folders for class_1 and class_2\n",
        "class_1_folder_train = '/content/drive/MyDrive/dataset/train/class_1/'\n",
        "class_2_folder_train = '/content/drive/MyDrive/dataset/train/class_2/'\n",
        "\n",
        "class_1_folder_val = '/content/drive/MyDrive/dataset/val/class_1/'\n",
        "class_2_folder_val = '/content/drive/MyDrive/dataset/val/class_2/'\n",
        "\n",
        "class_1_folder_test = '/content/drive/MyDrive/dataset/test/class_1/'\n",
        "class_2_folder_test = '/content/drive/MyDrive/dataset/test/class_2/'\n",
        "\n",
        "# Move images to corresponding folders based on class\n",
        "for img in os.listdir(source_folder):\n",
        "    if 'class_1' in img:  # Adjust this condition based on image naming\n",
        "        shutil.move(os.path.join(source_folder, img), os.path.join(class_1_folder_train, img))\n",
        "    elif 'class_2' in img:  # Adjust this condition based on image naming\n",
        "        shutil.move(os.path.join(source_folder, img), os.path.join(class_2_folder_train, img))\n",
        "\n",
        "# Verify the files are in place\n",
        "print(\"Train class_1:\", os.listdir(class_1_folder_train))\n",
        "print(\"Train class_2:\", os.listdir(class_2_folder_train))\n",
        "print(\"Val class_1:\", os.listdir(class_1_folder_val))\n",
        "print(\"Val class_2:\", os.listdir(class_2_folder_val))\n",
        "print(\"Test class_1:\", os.listdir(class_1_folder_test))\n",
        "print(\"Test class_2:\", os.listdir(class_2_folder_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBXDKqYqwIp4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "source_folder = '/content/drive/MyDrive/dataset/images/'  # Replace with the actual correct path\n",
        "print(os.listdir('/content/drive/MyDrive/dataset/'))  # Print out to ensure the images folder exists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQeWDcZOwNhX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check the contents of the train, val, and test directories\n",
        "print(\"Train folder contents:\", os.listdir('/content/drive/MyDrive/dataset/train'))\n",
        "print(\"Val folder contents:\", os.listdir('/content/drive/MyDrive/dataset/val'))\n",
        "print(\"Test folder contents:\", os.listdir('/content/drive/MyDrive/dataset/test'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mCxSnowwqWo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define your directory paths\n",
        "train_dir = '/content/drive/MyDrive/dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/dataset/val'\n",
        "test_dir = '/content/drive/MyDrive/dataset/test'\n",
        "\n",
        "# Create ImageDataGenerators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "# Flow images in batches from the respective directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),  # Adjust to your desired input size\n",
        "    batch_size=32,\n",
        "    class_mode='binary'      # 'binary' for 2 classes, 'categorical' for more\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li2vAqamw1sR"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/MyDrive/dataset/train'\n",
        "val_dir = '/content/drive/MyDrive/dataset/val'\n",
        "test_dir = '/content/drive/MyDrive/dataset/test'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiAETbjiw31D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Train folder contents:\", os.listdir('/content/drive/MyDrive/dataset/train'))\n",
        "print(\"Val folder contents:\", os.listdir('/content/drive/MyDrive/dataset/val'))\n",
        "print(\"Test folder contents:\", os.listdir('/content/drive/MyDrive/dataset/test'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxoWFG-Kw60j"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset/train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # or 'categorical' if you have more than 2 classes\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset/val',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/dataset/test',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHyNJBgGxW1q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir('<your_image_directory_path>'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG4pVQBPxwSQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7DS0pdIxyse"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBykUvLQx0qD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.listdir(dataset_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQoryvXeyAZO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/dataset'  # Example path, adjust to yours\n",
        "print(\"Contents of dataset folder:\", os.listdir(dataset_path))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDkt-sI9yKi2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/dataset'  # Or wherever your dataset is\n",
        "\n",
        "print(\"Inside 'train':\", os.listdir(os.path.join(dataset_path, 'train')))\n",
        "print(\"Inside 'val':\", os.listdir(os.path.join(dataset_path, 'val')))\n",
        "print(\"Inside 'test':\", os.listdir(os.path.join(dataset_path, 'test')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jfi3G2WJzgDl"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD_inPxhzpDw"
      },
      "outputs": [],
      "source": [
        "class_id  x_center  y_center  width  height\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOmII5A-0SJ6"
      },
      "outputs": [],
      "source": [
        "0 0.5 0.5 0.2 0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCyz5pt7923g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "\n",
        "# 1. Define paths (adjust these)\n",
        "dataset_path = '/content/drive/MyDrive/dataset'  # Your dataset root\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "val_path = os.path.join(dataset_path, 'val')  # If you have a validation set\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "# 2. Image dimensions (important for consistency)\n",
        "img_height, img_width = 224, 224  # Or any size you prefer\n",
        "\n",
        "# 3. Data Generators (for preprocessing and augmentation)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,  # Normalize pixel values (0-1)\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'  # Handle newly created pixels\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255) # Validation data should typically only be rescaled\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255) # Same for Test data\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,  # Adjust as needed\n",
        "    class_mode='categorical'  # Or 'binary' if you have only two classes\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Check the classes and number of images\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(\"Classes:\", train_generator.class_indices)  # Print class names and their indices\n",
        "print(\"Number of training images:\", train_generator.n)\n",
        "print(\"Number of validation images:\", validation_generator.n)\n",
        "print(\"Number of testing images:\", test_generator.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duiDvobH-AwO"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset path:\", dataset_path)\n",
        "print(\"Train path:\", train_path)\n",
        "print(\"Validation path:\", val_path)\n",
        "print(\"Test path:\", test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiRyAPtj-Oiw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"Contents of dataset folder:\", os.listdir(dataset_path))  # What's directly in 'dataset'?\n",
        "\n",
        "print(\"Contents of train folder:\", os.listdir(train_path))   # What's inside 'train'?\n",
        "for class_name in os.listdir(train_path):  # Check each class folder in 'train'\n",
        "    class_folder = os.path.join(train_path, class_name)\n",
        "    print(f\"Contents of {class_name} folder (train):\", os.listdir(class_folder))\n",
        "\n",
        "# REPEAT the above for val_path and test_path as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdtkHlozAG-a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil  # For moving files\n",
        "\n",
        "# 1. Configuration (REPLACE THESE WITH YOUR ACTUAL VALUES)\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train folder in Drive\n",
        "class_1_name = \"class_1\"  # Name of your first class subfolder\n",
        "class_2_name = \"class_2\"  # Name of your second class subfolder\n",
        "\n",
        "# 2. Function to determine the class (YOU MUST MODIFY THIS)\n",
        "def get_class(filename):\n",
        "    \"\"\"\n",
        "    This function determines the class of an image based on its filename.\n",
        "    YOU MUST MODIFY THIS LOGIC TO MATCH YOUR ACTUAL CRITERIA.\n",
        "\n",
        "    Example 1 (Even/Odd):\n",
        "    if int(filename.split(\"_\")[2]) % 2 == 0:  # Extract number from filename\n",
        "        return class_2_name\n",
        "    else:\n",
        "        return class_1_name\n",
        "\n",
        "    Example 2 (Contains \"polluted\"):\n",
        "    if \"polluted\" in filename:\n",
        "        return class_2_name  # Or whatever your \"polluted\" class name is\n",
        "    else:\n",
        "        return class_1_name\n",
        "\n",
        "    Example 3 (Based on .txt file content):\n",
        "    txt_filename = filename[:-4] + \".txt\"  # Replace .jpg with .txt\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "    with open(txt_filepath, 'r') as f:\n",
        "        content = f.read()\n",
        "        if \"some_keyword\" in content:\n",
        "            return class_2_name\n",
        "        else:\n",
        "            return class_1_name\n",
        "\n",
        "    \"\"\"\n",
        "    # *** REPLACE THE CODE BELOW WITH YOUR ACTUAL LOGIC ***\n",
        "    # This is just a placeholder example (even/odd):\n",
        "    if int(filename.split(\"_\")[2]) % 2 == 0:  # Extract number from filename\n",
        "        return class_2_name\n",
        "    else:\n",
        "        return class_1_name\n",
        "\n",
        "# 3. Process the files\n",
        "for filename in os.listdir(train_dir):\n",
        "    if filename.endswith(\".jpg\"):  # Process only image files\n",
        "        image_filepath = os.path.join(train_dir, filename)\n",
        "        txt_filename = filename[:-4] + \".txt\"  # Create .txt filename\n",
        "        xml_filename = filename[:-4] + \".xml\"  # Create .xml filename\n",
        "\n",
        "        txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "        xml_filepath = os.path.join(train_dir, xml_filename)\n",
        "\n",
        "        predicted_class = get_class(filename)  # Determine the class\n",
        "\n",
        "        # Move the files\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(train_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "        shutil.move(image_filepath, target_dir)\n",
        "        if os.path.exists(txt_filepath): shutil.move(txt_filepath, target_dir) #Check if file exists before moving\n",
        "        if os.path.exists(xml_filepath): shutil.move(xml_filepath, target_dir) #Check if file exists before moving\n",
        "\n",
        "print(\"Files moved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6BRxal-AQrQ"
      },
      "outputs": [],
      "source": [
        "def get_class(filename):\n",
        "    parts = filename.split(\"_\")\n",
        "    if len(parts) >= 3:  # Check if there are at least 3 parts\n",
        "        try:  # Try converting to integer in case it's not a number\n",
        "            number = int(parts[2])\n",
        "            if number % 2 == 0:\n",
        "                return class_2_name\n",
        "            else:\n",
        "                return class_1_name\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not convert to number: {filename}\")\n",
        "            return \"unknown\"  # Or handle differently\n",
        "    else:\n",
        "        print(f\"Warning: Filename does not match expected format: {filename}\")\n",
        "        return \"unknown\"  # Or handle differently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e2ATr2IATwE"
      },
      "outputs": [],
      "source": [
        "def get_class(filename):\n",
        "    try:\n",
        "        if int(filename.split(\"_\")[2]) % 2 == 0:\n",
        "            return class_2_name\n",
        "        else:\n",
        "            return class_1_name\n",
        "    except IndexError:\n",
        "        print(f\"Warning: Filename does not match expected format: {filename}\")\n",
        "        return \"unknown\"  # Or handle differently\n",
        "    except ValueError: #In case the split part is not a number\n",
        "        print(f\"Warning: Could not convert to number: {filename}\")\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgrfAAGUAWzS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def get_class(filename):\n",
        "    numbers = re.findall(r\"\\d+\", filename)  # Find all numbers in the filename\n",
        "    if numbers:\n",
        "        try:\n",
        "            last_number = int(numbers[-1])  # Get the last number\n",
        "            if last_number % 2 == 0:\n",
        "                return class_2_name\n",
        "            else:\n",
        "                return class_1_name\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not convert to number: {filename}\")\n",
        "            return \"unknown\"\n",
        "    else:\n",
        "        print(f\"Warning: No numbers found in filename: {filename}\")\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBylbKUjAmbH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_class(filename):\n",
        "    txt_filename = filename[:-4] + \".txt\"  # Replace .jpg with .txt\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename) #Use train_dir variable here.\n",
        "\n",
        "    if os.path.exists(txt_filepath): #Check if file exists\n",
        "        try:\n",
        "            with open(txt_filepath, 'r') as f:\n",
        "                content = f.read().strip()  # Read content and remove leading/trailing spaces\n",
        "\n",
        "                if content == \"1\":  # Or whatever indicates class 1 in your .txt file\n",
        "                    return class_1_name\n",
        "                elif content == \"2\":  # Or whatever indicates class 2 in your .txt file\n",
        "                    return class_2_name\n",
        "                else:\n",
        "                    print(f\"Warning: Unknown class indicator in {txt_filename}: {content}\")\n",
        "                    return \"unknown\"  # Or handle differently\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "            return \"unknown\"  # Or handle differently\n",
        "    else:\n",
        "        print(f\"Warning: .txt file not found for: {filename}\")\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evVKfRJTBElN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#... (your configuration - train_dir, class_1_name, class_2_name)\n",
        "\n",
        "def get_class(filename):\n",
        "    #... (your logic to determine the class - if you have any)\n",
        "\n",
        "    # If no class can be determined:\n",
        "    print(f\"Warning: Could not determine class for {filename}\")\n",
        "    return \"unknown\"  # Or handle differently (e.g., skip the file)\n",
        "\n",
        "#... (rest of the code to move the files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIenL0NXBX9S"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Configuration (REPLACE THESE WITH YOUR ACTUAL VALUES)\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train folder in Drive\n",
        "class_1_name = \"class_1\"  # Name of your first class subfolder\n",
        "class_2_name = \"class_2\"  # Name of your second class subfolder\n",
        "\n",
        "# 2. Function to determine the class (Corrected)\n",
        "def get_class(filename):\n",
        "    txt_filename = filename[:-4] + \".txt\"  # Replace .jpg with .txt\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "\n",
        "    if os.path.exists(txt_filepath):\n",
        "        try:\n",
        "            with open(txt_filepath, 'r') as f:\n",
        "                first_line = f.readline().strip()  # Read only the first line\n",
        "                class_label = first_line.split()[0] #Split the line by spaces and take the first element.\n",
        "\n",
        "                if class_label == \"1\":  # If the first number is \"1\"\n",
        "                    return class_1_name\n",
        "                elif class_label == \"2\":  # If the first number is \"2\"\n",
        "                    return class_2_name\n",
        "                else:\n",
        "                    print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                    return \"unknown\"\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "            return \"unknown\"\n",
        "        except IndexError:\n",
        "            print(f\"Warning: No class label found in {txt_filename}\") #If there is no number in the first line.\n",
        "            return \"unknown\"\n",
        "    else:\n",
        "        print(f\"Warning: .txt file not found for: {filename}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "# 3. Process the files\n",
        "for filename in os.listdir(train_dir):\n",
        "    if filename.endswith(\".jpg\"):  # Process only image files\n",
        "        image_filepath = os.path.join(train_dir, filename)\n",
        "        txt_filename = filename[:-4] + \".txt\"  # Create .txt filename\n",
        "        xml_filename = filename[:-4] + \".xml\"  # Create .xml filename\n",
        "\n",
        "        txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "        xml_filepath = os.path.join(train_dir, xml_filename)\n",
        "\n",
        "        predicted_class = get_class(filename)  # Determine the class\n",
        "\n",
        "        # Move the files (only if class is known)\n",
        "        if predicted_class != \"unknown\":\n",
        "            if predicted_class == class_1_name:\n",
        "                target_dir = os.path.join(train_dir, class_1_name)\n",
        "            else:\n",
        "                target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if os.path.exists(txt_filepath): shutil.move(txt_filepath, target_dir)\n",
        "            if os.path.exists(xml_filepath): shutil.move(xml_filepath, target_dir)\n",
        "        else:\n",
        "            print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "print(\"Files moved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3McIJfDCWat"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Configuration (REPLACE THESE WITH YOUR ACTUAL VALUES)\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train folder in Drive\n",
        "class_1_name = \"class_1\"  # Name of your first class subfolder\n",
        "class_2_name = \"class_2\"  # Name of your second class subfolder\n",
        "\n",
        "# 2. Function to determine the class (Corrected)\n",
        "def get_class(filename):\n",
        "    txt_filename = filename[:-4] + \".txt\"  # Replace .jpg with .txt\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "\n",
        "    try:\n",
        "        with open(txt_filepath, 'r') as f:\n",
        "            first_line = f.readline().strip()\n",
        "            class_label = first_line.split()[0]\n",
        "\n",
        "            if class_label == \"1\":\n",
        "                return class_1_name\n",
        "            elif class_label == \"2\":\n",
        "                return class_2_name\n",
        "            else:\n",
        "                print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                return \"unknown\"\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except IndexError:\n",
        "        print(f\"Warning: No class label found in {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except Exception as e: # Catch other exceptions to prevent the script from crashing.\n",
        "        print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "# 3. Process the files (OPTIMIZED)\n",
        "# Create sets of existing .txt and .xml files for fast lookups\n",
        "existing_txt_files = set(os.listdir(train_dir)) #Use set for O(1) lookup time.\n",
        "existing_xml_files = set(os.listdir(train_dir)) #Use set for O(1) lookup time.\n",
        "\n",
        "for filename in os.listdir(train_dir):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        image_filepath = os.path.join(train_dir, filename)\n",
        "        txt_filename = filename[:-4] + \".txt\"\n",
        "        xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "        predicted_class = get_class(filename)\n",
        "\n",
        "        if predicted_class != \"unknown\":\n",
        "            if predicted_class == class_1_name:\n",
        "                target_dir = os.path.join(train_dir, class_1_name)\n",
        "            else:\n",
        "                target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files: shutil.move(os.path.join(train_dir, txt_filename), target_dir) #Check in set.\n",
        "            if xml_filename in existing_xml_files: shutil.move(os.path.join(train_dir, xml_filename), target_dir) #Check in set.\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "print(\"Files moved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go1ypXC6Co-V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "\n",
        "# ... (Configuration - train_dir, class_1_name, class_2_name)\n",
        "\n",
        "# ... (get_class() function - same as the optimized version)\n",
        "\n",
        "def move_files(filename):  # Function to process a single file (for parallelization)\n",
        "    image_filepath = os.path.join(train_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename)\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(train_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files: shutil.move(os.path.join(train_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files: shutil.move(os.path.join(train_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "if __name__ == '__main__':  # Important for multiprocessing on Windows\n",
        "    # Create sets of existing .txt and .xml files (outside the parallel loop)\n",
        "    existing_txt_files = set(os.listdir(train_dir))\n",
        "    existing_xml_files = set(os.listdir(train_dir))\n",
        "\n",
        "    jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:  # Use all available CPU cores\n",
        "        pool.map(move_files, jpg_files)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOboDX-8CxAl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "\n",
        "# 1. Configuration (REPLACE THESE WITH YOUR ACTUAL VALUES)\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train folder in Drive\n",
        "class_1_name = \"class_1\"  # Name of your first class subfolder\n",
        "class_2_name = \"class_2\"  # Name of your second class subfolder\n",
        "\n",
        "# 2. Function to determine the class (Corrected)\n",
        "def get_class(filename):\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "\n",
        "    try:\n",
        "        with open(txt_filepath, 'r') as f:\n",
        "            first_line = f.readline().strip()\n",
        "            class_label = first_line.split()[0]\n",
        "\n",
        "            if class_label == \"1\":\n",
        "                return class_1_name\n",
        "            elif class_label == \"2\":\n",
        "                return class_2_name\n",
        "            else:\n",
        "                print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                return \"unknown\"\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except IndexError:\n",
        "        print(f\"Warning: No class label found in {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "# 3. Process the files (OPTIMIZED and CORRECTED SCOPE)\n",
        "if __name__ == '__main__':\n",
        "    # Configuration (inside the if block)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train folder in Drive\n",
        "    class_1_name = \"class_1\"  # Name of your first class subfolder\n",
        "    class_2_name = \"class_2\"  # Name of your second class subfolder\n",
        "\n",
        "    # Create sets of existing .txt and .xml files (outside the parallel loop)\n",
        "    existing_txt_files = set(os.listdir(train_dir))\n",
        "    existing_xml_files = set(os.listdir(train_dir))\n",
        "\n",
        "    jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.map(move_files, jpg_files)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "\n",
        "def move_files(filename):  # Function to process a single file (for parallelization)\n",
        "    image_filepath = os.path.join(train_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename)\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(train_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files: shutil.move(os.path.join(train_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files: shutil.move(os.path.join(train_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVIYG1cbC_fK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "\n",
        "def get_class(filename):  # (Same as before - reads .txt file)\n",
        "    # ... (your get_class() function code here)\n",
        "\n",
        "def move_files(filename):  # Function for parallel processing\n",
        "    image_filepath = os.path.join(train_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename)\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(train_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files: shutil.move(os.path.join(train_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files: shutil.move(os.path.join(train_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "if __name__ == '__main__': # CRUCIAL: Multiprocessing code MUST be here\n",
        "    # 1. Configuration (INSIDE the if block)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"  # Replace with your actual path\n",
        "    class_1_name = \"class_1\"  # Replace with your actual class name\n",
        "    class_2_name = \"class_2\"  # Replace with your actual class name\n",
        "\n",
        "    # 2. Create sets of existing files (for optimization)\n",
        "    existing_txt_files = set(os.listdir(train_dir))\n",
        "    existing_xml_files = set(os.listdir(train_dir))\n",
        "\n",
        "    # 3. Get list of .jpg files\n",
        "    jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    # 4. Parallel processing\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.map(move_files, jpg_files)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYpnGDyjDFkC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "\n",
        "def get_class(filename):  # (Same as before - reads .txt file)\n",
        "    # ... (your get_class() function code here)\n",
        "\n",
        "def move_files(filename):  # Function for parallel processing\n",
        "    image_filepath = os.path.join(train_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename)\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(train_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files: shutil.move(os.path.join(train_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files: shutil.move(os.path.join(train_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "if __name__ == '__main__': # CRUCIAL: Multiprocessing code MUST be here\n",
        "    # 1. Configuration (INSIDE the if block)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"  # Replace with your actual path\n",
        "    class_1_name = \"class_1\"  # Replace with your actual class name\n",
        "    class_2_name = \"class_2\"  # Replace with your actual class name\n",
        "\n",
        "    # 2. Create sets of existing files (for optimization)\n",
        "    existing_txt_files = set(os.listdir(train_dir))\n",
        "    existing_xml_files = set(os.listdir(train_dir))\n",
        "\n",
        "    # 3. Get list of .jpg files\n",
        "    jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    # 4. Parallel processing\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.map(move_files, jpg_files)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgpN6XX-DMRz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "\n",
        "def get_class(filename):  # (Same as before - reads .txt file)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "\n",
        "    try:\n",
        "        with open(txt_filepath, 'r') as f:\n",
        "            first_line = f.readline().strip()\n",
        "            class_label = first_line.split()[0]\n",
        "\n",
        "            if class_label == \"1\":\n",
        "                return class_1_name\n",
        "            elif class_label == \"2\":\n",
        "                return class_2_name\n",
        "            else:\n",
        "                print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                return \"unknown\"\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except IndexError:\n",
        "        print(f\"Warning: No class label found in {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "def move_files(filename):  # Function for parallel processing\n",
        "    image_filepath = os.path.join(train_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename)\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(train_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(train_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files:\n",
        "                shutil.move(os.path.join(train_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files:\n",
        "                shutil.move(os.path.join(train_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__': # CRUCIAL: Multiprocessing code MUST be here\n",
        "    # 1. Configuration (INSIDE the if block)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"  # Replace with your actual path\n",
        "    class_1_name = \"class_1\"  # Replace with your actual class name\n",
        "    class_2_name = \"class_2\"  # Replace with your actual class name\n",
        "\n",
        "    # 2. Create sets of existing files (for optimization)\n",
        "    existing_txt_files = set(os.listdir(train_dir))\n",
        "    existing_xml_files = set(os.listdir(train_dir))\n",
        "\n",
        "    # 3. Get list of .jpg files\n",
        "    jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    # 4. Parallel processing\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.map(move_files, jpg_files)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc2NG299Faml"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# 1. Data Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)  # Rescale pixel values to 0-1\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,  # Your training directory\n",
        "    target_size=(150, 150),  # Resize images to 150x150\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Use categorical for multiple classes\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,  # Your validation directory (REPLACE with your validation directory)\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,  # Your test directory (REPLACE with your test directory)\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# 2. Build the Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Add dropout for regularization\n",
        "    Dense(train_generator.num_classes, activation='softmax')  # Output layer with softmax\n",
        "])\n",
        "\n",
        "# 3. Compile the Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use categorical cross-entropy\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the Model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,  # Adjust the number of epochs as needed\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# 6. Save the Model (Optional)\n",
        "model.save(\"your_model_name.h5\")  # Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiZvyjyTFnB4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# 1. Define Directory Variables (REPLACE THESE WITH YOUR ACTUAL PATHS)\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train directory\n",
        "val_dir = \"/content/drive/MyDrive/dataset/val\"      # Path to your validation directory\n",
        "test_dir = \"/content/drive/MyDrive/dataset/test\"    # Path to your test directory\n",
        "\n",
        "# 2. Data Generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# ... (Rest of your model building, training, and evaluation code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd_WhcQJFyhp"
      },
      "outputs": [],
      "source": [
        "# ... (get_class() function - same as before)\n",
        "\n",
        "# ... (move_files() function - same as before)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Configuration for train\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    class_1_name = \"class_1\"\n",
        "    class_2_name = \"class_2\"\n",
        "\n",
        "    # Configuration for val\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"  # Replace with your actual path\n",
        "    val_existing_txt_files = set(os.listdir(val_dir)) #Create a set of existing txt files for val\n",
        "    val_existing_xml_files = set(os.listdir(val_dir)) #Create a set of existing xml files for val\n",
        "\n",
        "    # Configuration for test\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"  # Replace with your actual path\n",
        "    test_existing_txt_files = set(os.listdir(test_dir)) #Create a set of existing txt files for test\n",
        "    test_existing_xml_files = set(os.listdir(test_dir)) #Create a set of existing xml files for test\n",
        "\n",
        "\n",
        "    # ... (File moving logic for train - same as before)\n",
        "\n",
        "    # File moving logic for val (adapt as needed)\n",
        "    val_jpg_files = [f for f in os.listdir(val_dir) if f.endswith(\".jpg\")]\n",
        "    for filename in val_jpg_files:\n",
        "        move_files(filename) #Call move files for val as well.\n",
        "\n",
        "    # File moving logic for test (adapt as needed)\n",
        "    test_jpg_files = [f for f in os.listdir(test_dir) if f.endswith(\".jpg\")]\n",
        "    for filename in test_jpg_files:\n",
        "        move_files(filename) #Call move files for test as well.\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogixPWqxGQsL"
      },
      "outputs": [],
      "source": [
        "def get_class(filename):\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    txt_filepath = os.path.join(train_dir, txt_filename)\n",
        "\n",
        "    if os.path.exists(txt_filepath):\n",
        "        try:\n",
        "            with open(txt_filepath, 'r') as f:\n",
        "                first_line = f.readline().strip()\n",
        "                class_label = first_line.split()[0]\n",
        "\n",
        "                if class_label == \"1\":\n",
        "                    return class_1_name\n",
        "                elif class_label == \"2\":\n",
        "                    return class_2_name\n",
        "                else:\n",
        "                    print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                    return \"unknown\"\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "            return \"unknown\"  # Or return a default class, e.g., return class_1_name\n",
        "        except IndexError:\n",
        "            print(f\"Warning: No class label found in {txt_filename}\")\n",
        "            return \"unknown\"  # Or return a default class\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "            return \"unknown\"\n",
        "    else:\n",
        "        print(f\"Warning: .txt file not found for: {filename}\")\n",
        "        # Return a default class here if appropriate:\n",
        "        # return class_1_name  # Example: assign to class 1 if .txt is missing\n",
        "        return \"unknown\"  # Or skip the file as in the original code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXxYsEd-GoVi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def get_class(filename, data_dir):  # Add data_dir argument\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    txt_filepath = os.path.join(data_dir, txt_filename)  # Use data_dir\n",
        "\n",
        "    try:\n",
        "        with open(txt_filepath, 'r') as f:\n",
        "            # ... (rest of your get_class function - same as before)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except IndexError:\n",
        "        print(f\"Warning: No class label found in {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "def move_files(filename, data_dir, existing_txt_files, existing_xml_files):  # Add arguments\n",
        "    image_filepath = os.path.join(data_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename, data_dir)  # Pass data_dir\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(data_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(data_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files:\n",
        "                shutil.move(os.path.join(data_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files:\n",
        "                shutil.move(os.path.join(data_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Configuration (inside if __main__ block)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "    class_1_name = \"class_1\"\n",
        "    class_2_name = \"class_2\"\n",
        "\n",
        "    # Create sets of existing files (once, outside the loop)\n",
        "    train_existing_txt = set(os.listdir(train_dir))\n",
        "    train_existing_xml = set(os.listdir(train_dir))\n",
        "    val_existing_txt = set(os.listdir(val_dir))\n",
        "    val_existing_xml = set(os.listdir(val_dir))\n",
        "    test_existing_txt = set(os.listdir(test_dir))\n",
        "    test_existing_xml = set(os.listdir(test_dir))\n",
        "\n",
        "    # Process train data\n",
        "    train_jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, train_dir, train_existing_txt, train_existing_xml) for f in train_jpg_files])\n",
        "\n",
        "    # Process val data\n",
        "    val_jpg_files = [f for f in os.listdir(val_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, val_dir, val_existing_txt, val_existing_xml) for f in val_jpg_files])\n",
        "\n",
        "    # Process test data\n",
        "    test_jpg_files = [f for f in os.listdir(test_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, test_dir, test_existing_txt, test_existing_xml) for f in test_jpg_files])\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Rest of your data loading and model training code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knzDUxoIGwjD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def get_class(filename, data_dir):  # Add data_dir argument\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    txt_filepath = os.path.join(data_dir, txt_filename)  # Use data_dir\n",
        "\n",
        "    try:\n",
        "        with open(txt_filepath, 'r') as f:  # Correct indentation here\n",
        "            first_line = f.readline().strip()\n",
        "            class_label = first_line.split()[0]\n",
        "\n",
        "            if class_label == \"1\":\n",
        "                return class_1_name\n",
        "            elif class_label == \"2\":\n",
        "                return class_2_name\n",
        "            else:\n",
        "                print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                return \"unknown\"\n",
        "\n",
        "    except FileNotFoundError:  # Correct indentation here\n",
        "        print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except IndexError:  # Correct indentation here\n",
        "        print(f\"Warning: No class label found in {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except Exception as e:  # Correct indentation here\n",
        "        print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "# ... (rest of the code - move_files() function, if __name__ == '__main__' block, etc. - same as in the previous corrected response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PXgMqGPG47I"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def get_class(filename, data_dir):  # Add data_dir argument\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    txt_filepath = os.path.join(data_dir, txt_filename)  # Use data_dir\n",
        "\n",
        "    try:\n",
        "        with open(txt_filepath, 'r') as f:  # Correct indentation here\n",
        "            first_line = f.readline().strip()\n",
        "            class_label = first_line.split()[0]\n",
        "\n",
        "            if class_label == \"1\":\n",
        "                return class_1_name\n",
        "            elif class_label == \"2\":\n",
        "                return class_2_name\n",
        "            else:\n",
        "                print(f\"Warning: Unknown class label in {txt_filename}: {class_label}\")\n",
        "                return \"unknown\"\n",
        "\n",
        "    except FileNotFoundError:  # Correct indentation here\n",
        "        print(f\"Warning: .txt file not found: {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except IndexError:  # Correct indentation here\n",
        "        print(f\"Warning: No class label found in {txt_filename}\")\n",
        "        return \"unknown\"\n",
        "    except Exception as e:  # Correct indentation here\n",
        "        print(f\"An unexpected error occurred with {txt_filename}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "def move_files(filename, data_dir, existing_txt_files, existing_xml_files):  # Add arguments\n",
        "    image_filepath = os.path.join(data_dir, filename)\n",
        "    txt_filename = filename[:-4] + \".txt\"\n",
        "    xml_filename = filename[:-4] + \".xml\"\n",
        "\n",
        "    predicted_class = get_class(filename, data_dir)  # Pass data_dir\n",
        "\n",
        "    if predicted_class != \"unknown\":\n",
        "        if predicted_class == class_1_name:\n",
        "            target_dir = os.path.join(data_dir, class_1_name)\n",
        "        else:\n",
        "            target_dir = os.path.join(data_dir, class_2_name)\n",
        "\n",
        "        try:\n",
        "            shutil.move(image_filepath, target_dir)\n",
        "            if txt_filename in existing_txt_files:\n",
        "                shutil.move(os.path.join(data_dir, txt_filename), target_dir)\n",
        "            if xml_filename in existing_xml_files:\n",
        "                shutil.move(os.path.join(data_dir, xml_filename), target_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error moving {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"Warning: Skipping {filename} due to unknown class.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Configuration (inside if __main__ block)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "    class_1_name = \"class_1\"\n",
        "    class_2_name = \"class_2\"\n",
        "\n",
        "    # Create sets of existing files (once, outside the loop)\n",
        "    train_existing_txt = set(os.listdir(train_dir))\n",
        "    train_existing_xml = set(os.listdir(train_dir))\n",
        "    val_existing_txt = set(os.listdir(val_dir))\n",
        "    val_existing_xml = set(os.listdir(val_dir))\n",
        "    test_existing_txt = set(os.listdir(test_dir))\n",
        "    test_existing_xml = set(os.listdir(test_dir))\n",
        "\n",
        "    # Process train data\n",
        "    train_jpg_files = [f for f in os.listdir(train_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, train_dir, train_existing_txt, train_existing_xml) for f in train_jpg_files])\n",
        "\n",
        "    # Process val data\n",
        "    val_jpg_files = [f for f in os.listdir(val_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, val_dir, val_existing_txt, val_existing_xml) for f in val_jpg_files])\n",
        "\n",
        "    # Process test data\n",
        "    test_jpg_files = [f for f in os.listdir(test_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, test_dir, test_existing_txt, test_existing_xml) for f in test_jpg_files])\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Rest of your data loading and model training code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxbP0P6HJK9q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# ... (get_class() and move_files() functions - same as the corrected version)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic - same as the corrected version)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Data Loading and Model Training (NEW CODE STARTING HERE)\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # 1. Data Generators (REPLACE PATHS IF NEEDED)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"  # Path to your train directory\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"      # Path to your validation directory\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"    # Path to your test directory\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),  # Adjust as needed\n",
        "        batch_size=32,          # Adjust as needed\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # 2. Build the Model (Adjust architecture as needed)\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(train_generator.num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # 3. Compile the Model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 4. Train the Model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "        epochs=10,  # Adjust as needed\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate the Model\n",
        "    loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "    print(f\"Test Loss: {loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # 6. Save the Model (Optional)\n",
        "    model.save(\"your_model_name.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL2s3NaXKR2V"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of training samples: {train_generator.samples}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFKWy55QKZzd"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of validation samples: {validation_generator.samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWBnxjrOK_LC"
      },
      "outputs": [],
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=16,  # Reduce batch size for validation\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXBH7SQ5LFD0"
      },
      "outputs": [],
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=16,  # Or even smaller, like 8 or 4\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kIlBKafLQRl"
      },
      "outputs": [],
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=16,  # Or even smaller, like 8 or 4\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yVtuDEdLc8g"
      },
      "outputs": [],
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=16,  # Or 8, 4, or 2\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jv3-nNkLgdB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# ... (get_class() and move_files() functions - same as the corrected version with indentation fix)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic - same as the corrected version with starmap)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Data Loading and Model Training\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # 1. Data Generators (REPLACE PATHS IF NEEDED)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=16,  # Reduced batch size for validation\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # 2. Build the Model (Adjust architecture as needed)\n",
        "    model = Sequential([\n",
        "        # ... (your model architecture)\n",
        "    ])\n",
        "\n",
        "    # 3. Compile the Model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 4. Train the Model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "        epochs=10,  # Adjust as needed\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // validation_generator.batch_size  # Use the smaller batch size\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate the Model\n",
        "    loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "    print(f\"Test Loss: {loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # 6. Save the Model (Optional)\n",
        "    model.save(\"your_model_name.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETw0ejteLqE8"
      },
      "outputs": [],
      "source": [
        "for data, labels in train_generator:\n",
        "    print(data.shape)\n",
        "    break  # Exit the loop after printing the shape once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKugCJsFL1w9"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),  # Double-check this line\n",
        "    MaxPooling2D((2, 2)),\n",
        "    # ... (rest of your model layers)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBxrgSj1L25h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "#... (get_class() and move_files() functions - same as the corrected version with indentation fix)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    #... (File moving logic for train and val - same as the corrected version with starmap)\n",
        "\n",
        "    # File moving logic for test (corrected for subfolders)\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "    test_existing_txt = set(os.listdir(test_dir))\n",
        "    test_existing_xml = set(os.listdir(test_dir))\n",
        "\n",
        "    test_jpg_files = [f for f in os.listdir(test_dir) if f.endswith(\".jpg\")]\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        pool.starmap(move_files, [(f, test_dir, test_existing_txt, test_existing_xml) for f in test_jpg_files])\n",
        "\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Data Loading and Model Training\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # 1. Data Generators (REPLACE PATHS IF NEEDED)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=16,  # Reduced batch size for validation (or adjust validation_steps)\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # 2. Build the Model (Adjust architecture as needed)\n",
        "    model = Sequential([\n",
        "        #... (your model architecture)\n",
        "    ])\n",
        "\n",
        "    # 3. Compile the Model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 4. Train the Model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "        epochs=10,  # Adjust as needed\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // validation_generator.batch_size # Use the smaller batch size or validation_steps\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate the Model\n",
        "    loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "    print(f\"Test Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1W7j1FaMkLd"
      },
      "outputs": [],
      "source": [
        "# ... (rest of your code)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # Make absolutely sure this is 'categorical'\n",
        ")\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "model = Sequential([\n",
        "    # ... (your model layers)\n",
        "    Dense(train_generator.num_classes, activation='softmax')  # Correct output layer\n",
        "])\n",
        "\n",
        "# ... (rest of your code)\n",
        "\n",
        "# Check number of classes\n",
        "print(f\"Number of classes (train): {train_generator.num_classes}\")\n",
        "print(f\"Number of classes (validation): {validation_generator.num_classes}\")\n",
        "print(f\"Number of classes (test): {test_generator.num_classes}\")\n",
        "\n",
        "# Check label shape from generator\n",
        "for data, labels in train_generator:\n",
        "    print(f\"Label shape: {labels.shape}\") # Should be (batch_size, num_classes)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihVk1hUONBpp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# ... (get_class() and move_files() functions - corrected versions)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic - corrected versions)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Data Loading and Model Training\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # 1. Data Generators\n",
        "    # ... (same as before)\n",
        "\n",
        "    # 2. Build the Model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        # ... (rest of your model layers)\n",
        "        Dense(train_generator.num_classes, activation='softmax')  # Output layer\n",
        "    ])\n",
        "\n",
        "    # 3. Compile the Model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 4. Train the Model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "        epochs=10,  # Adjust as needed\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=validation_generator.samples // validation_generator.batch_size # or a smaller value if you want to use less validation data\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate the Model\n",
        "    loss, accuracy = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "    print(f\"Test Loss: {loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # 6. Save the Model (Optional)\n",
        "    model.save(\"your_model_name.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-pvaUhsNKPj"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    Flatten(),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN19JvHvNMsc"
      },
      "outputs": [],
      "source": [
        "for data, labels in train_generator:\n",
        "    print(f\"Data type: {data.dtype}\")\n",
        "    print(f\"Label type: {labels.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neoPiUkNNS0j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical  # Import for one-hot encoding\n",
        "import numpy as np\n",
        "\n",
        "# ... (get_class() and move_files() functions - corrected versions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data_dir, batch_size, target_size, num_classes):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for file in batch_files:\n",
        "            image_path = os.path.join(self.data_dir, file)\n",
        "            img = tf.keras.preprocessing.image.load_img(image_path, target_size=self.target_size)\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize\n",
        "            batch_images.append(img_array)\n",
        "\n",
        "            class_label = get_class(file, self.data_dir)  # Get class label from txt\n",
        "            if class_label == class_1_name:\n",
        "                label_index = 0\n",
        "            elif class_label == class_2_name:\n",
        "                label_index = 1\n",
        "            else:\n",
        "                label_index = -1  # or handle unknown labels differently\n",
        "                print(f\"Warning: Unknown class for {file}\")\n",
        "\n",
        "            batch_labels.append(label_index)\n",
        "\n",
        "        batch_images = np.array(batch_images)\n",
        "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes) # One-hot encode\n",
        "\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic - corrected versions)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Data Loading and Model Training\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # 1. Data Generators using the custom class\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "    class_1_name = \"class_1\"\n",
        "    class_2_name = \"class_2\"\n",
        "\n",
        "    num_classes = 2  # Define the number of classes\n",
        "\n",
        "    train_generator = CustomDataGenerator(train_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "    validation_generator = CustomDataGenerator(val_dir, batch_size=16, target_size=(150, 150), num_classes=num_classes)\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "\n",
        "\n",
        "    # 2. Build the Model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')  # Output layer\n",
        "    ])\n",
        "\n",
        "    # 3. Compile the Model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 4. Train the Model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,  # Adjust as needed\n",
        "        validation_data=validation_generator\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate the Model\n",
        "    loss, accuracy = model.evaluate(test_generator)\n",
        "    print(f\"Test Loss: {loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # 6. Save the Model (Optional)\n",
        "    model.save(\"your_model_name.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlg3pa-JTnr3"
      },
      "outputs": [],
      "source": [
        "# ... (after training)\n",
        "\n",
        "# Evaluate on the test set\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# (Optional) Get predictions and calculate confusion matrix, etc.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "test_generator.reset()  # Important: Reset the generator before making predictions\n",
        "y_true = test_generator.classes\n",
        "y_pred_probabilities = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred_probabilities, axis=1)  # Get predicted class indices\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# ... (further analysis and improvements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXxdNDzbsuhV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# ... (get_class() and move_files() functions - corrected versions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data_dir, batch_size, target_size, num_classes):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
        "        self.index = 0  # Add an index to keep track of position\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for file in batch_files:\n",
        "            image_path = os.path.join(self.data_dir, file)\n",
        "            img = tf.keras.preprocessing.image.load_img(image_path, target_size=self.target_size)\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0  # Normalize\n",
        "            batch_images.append(img_array)\n",
        "\n",
        "            class_label = get_class(file, self.data_dir)  # Get class label from txt\n",
        "            if class_label == class_1_name:\n",
        "                label_index = 0\n",
        "            elif class_label == class_2_name:\n",
        "                label_index = 1\n",
        "            else:\n",
        "                label_index = -1  # or handle unknown labels differently\n",
        "                print(f\"Warning: Unknown class for {file}\")\n",
        "\n",
        "            batch_labels.append(label_index)\n",
        "\n",
        "        batch_images = np.array(batch_images)\n",
        "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "    def reset(self):\n",
        "        self.index = 0  # Reset the index\n",
        "        np.random.shuffle(self.image_files) # shuffle the data after each epoch\n",
        "\n",
        "\n",
        "# ... (rest of your code, including the model training and evaluation)\n",
        "\n",
        "# ... (after training)\n",
        "\n",
        "test_generator.reset()  # Now the reset() method exists\n",
        "y_true = test_generator.classes\n",
        "y_pred_probabilities = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# ... (further analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvHNlULquDx9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... (other imports)\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition - same as the corrected version)\n",
        "\n",
        "# Define test_generator *outside* the if __name__ == '__main__' block or make it global\n",
        "test_generator = None  # Initialize it to None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Data Loading and Model Training\n",
        "    # ----------------------------------------------------------------------\n",
        "\n",
        "    # 1. Data Generators (REPLACE PATHS IF NEEDED)\n",
        "    train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "    val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "    test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "    class_1_name = \"class_1\"\n",
        "    class_2_name = \"class_2\"\n",
        "\n",
        "    num_classes = 2\n",
        "\n",
        "    train_generator = CustomDataGenerator(train_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "    validation_generator = CustomDataGenerator(val_dir, batch_size=16, target_size=(150, 150), num_classes=num_classes)\n",
        "    global test_generator # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes) # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # 5. Evaluate the Model\n",
        "    loss, accuracy = model.evaluate(test_generator)\n",
        "    print(f\"Test Loss: {loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "    # ... (other evaluation code)\n",
        "\n",
        "# Now test_generator is defined and accessible here\n",
        "test_generator.reset()  # This should work now\n",
        "y_true = test_generator.classes\n",
        "y_pred_probabilities = model.predict(test_generator)\n",
        "y_pred = np.argmax(y_pred_probabilities, axis=1)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P825UHiVuWZx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... other imports\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition - same as the corrected version with reset())\n",
        "\n",
        "test_generator = None  # Correct indentation: outside the class definition\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Data loading and model training)\n",
        "\n",
        "    global test_generator # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes) # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # ... (Evaluation code - same as before)\n",
        "\n",
        "# ... (rest of the code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWwiP193ugb4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... other imports\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition - same as the corrected version with reset())\n",
        "\n",
        "test_generator = None  # Correct indentation: at the TOP LEVEL of the script\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Data loading and model training)\n",
        "\n",
        "    global test_generator # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes) # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # ... (Evaluation code - same as before)\n",
        "\n",
        "# ... (rest of the code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOTpXmwuupJB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... other imports\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition - same as the corrected version with reset())\n",
        "\n",
        "test_generator = None  # Correct indentation: at the TOP LEVEL of the script (NO INDENTATION)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Data loading and model training)\n",
        "\n",
        "    global test_generator # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes) # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # ... (Evaluation code - same as before)\n",
        "\n",
        "# ... (rest of the code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgg5WME0uyCM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... other imports\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition - same as the corrected version with reset())\n",
        "\n",
        "    test_generator = None  # NO INDENTATION HERE - TOP LEVEL OF SCRIPT\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Data loading and model training)\n",
        "\n",
        "    global test_generator  # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)  # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # ... (Evaluation code - same as before)\n",
        "\n",
        "# ... (rest of the code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AqBgdl1vCDU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... other imports\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition)\n",
        "\n",
        "test_generator = None  # Correct indentation: at the TOP LEVEL of the script\n",
        "\n",
        "# Define test_dir *outside* the if __name__ == '__main__' block\n",
        "test_dir = \"/content/drive/MyDrive/dataset/test\"  # Define test_dir here\n",
        "class_1_name = \"class_1\"\n",
        "class_2_name = \"class_2\"\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Data loading and model training)\n",
        "\n",
        "    global test_generator  # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)  # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # ... (Evaluation code - same as before)\n",
        "\n",
        "# ... (rest of the code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTDfQbocver9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# ... other imports\n",
        "\n",
        "# ... (get_class() and move_files() functions)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    # ... (CustomDataGenerator class definition - same as the corrected version with reset())\n",
        "\n",
        "    test_generator = None  # NO INDENTATION HERE - TOP LEVEL OF SCRIPT\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (File moving logic)\n",
        "\n",
        "    print(\"Files moved (or skipped) successfully!\")\n",
        "\n",
        "    # ... (Data loading and model training)\n",
        "\n",
        "    global test_generator  # make it global so you can call it outside if condition\n",
        "    test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)  # assign value to the global test_generator\n",
        "\n",
        "    # ... (Model definition, compilation, and training)\n",
        "\n",
        "    # ... (Evaluation code - same as before)\n",
        "\n",
        "# ... (rest of the code - same as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqeNsBijvt4A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Define your data directories\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "\n",
        "# Define class names (replace with your actual class names)\n",
        "class_1_name = \"class_1\"\n",
        "class_2_name = \"class_2\"\n",
        "\n",
        "num_classes = 2  # Number of classes\n",
        "\n",
        "def get_class(filename, data_dir):\n",
        "    # Extract class label from filename or associated .txt/.xml files\n",
        "    # (Implement your logic here based on your file naming convention)\n",
        "    # This example assumes class is in subdirectory name\n",
        "    subdirectory = filename.split(os.sep)[-2] # Get parent directory which is the class name\n",
        "    return subdirectory\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data_dir, batch_size, target_size, num_classes):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.image_files = [os.path.join(root, file) for root, _, files in os.walk(data_dir) for file in files if file.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for file in batch_files:\n",
        "            img = load_img(file, target_size=self.target_size)\n",
        "            img_array = img_to_array(img) / 255.0\n",
        "            batch_images.append(img_array)\n",
        "\n",
        "            class_label = get_class(file, self.data_dir)\n",
        "            if class_label == class_1_name:\n",
        "                label_index = 0\n",
        "            elif class_label == class_2_name:\n",
        "                label_index = 1\n",
        "            else:\n",
        "                label_index = -1  # Handle unknown labels as needed\n",
        "                print(f\"Warning: Unknown class for {file}\")\n",
        "\n",
        "            batch_labels.append(label_index)\n",
        "\n",
        "        batch_images = np.array(batch_images)\n",
        "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "# Create data generators\n",
        "train_generator = CustomDataGenerator(train_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "validation_generator = CustomDataGenerator(val_dir, batch_size=16, target_size=(150, 150), num_classes=num_classes)\n",
        "test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Confusion Matrix (after training)\n",
        "test_generator.image_files = [os.path.join(root, file) for root, _, files in os.walk(test_dir) for file in files if file.endswith(\".jpg\")] # Refresh file list so that it is in order\n",
        "y_true = []\n",
        "y_pred = []\n",
        "for i in range(len(test_generator)):\n",
        "    x, y = test_generator[i]\n",
        "    y_true.extend(np.argmax(y, axis=1))\n",
        "    y_pred_probabilities = model.predict(x)\n",
        "    y_pred.extend(np.argmax(y_pred_probabilities, axis=1))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPKwi2Q-v3kS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# ... (get_class function - corrected as needed)\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data_dir, batch_size, target_size, num_classes, class_1_name, class_2_name):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.class_1_name = class_1_name\n",
        "        self.class_2_name = class_2_name\n",
        "        self.image_files = [] # Initialize as empty list\n",
        "        for root, _, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(\".jpg\"):\n",
        "                    self.image_files.append(os.path.join(root, file))\n",
        "\n",
        "        if not self.image_files: # Check if the list is empty\n",
        "            raise ValueError(f\"No .jpg images found in directory: {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for file in batch_files:\n",
        "            img = load_img(file, target_size=self.target_size)\n",
        "            img_array = img_to_array(img) / 255.0\n",
        "            batch_images.append(img_array)\n",
        "\n",
        "            class_label = get_class(file, self.data_dir)\n",
        "            if class_label == self.class_1_name:\n",
        "                label_index = 0\n",
        "            elif class_label == self.class_2_name:\n",
        "                label_index = 1\n",
        "            else:\n",
        "                label_index = -1  # Handle unknown labels as needed\n",
        "                print(f\"Warning: Unknown class for {file}\")\n",
        "\n",
        "            batch_labels.append(label_index)\n",
        "\n",
        "        batch_images = np.array(batch_images)\n",
        "\n",
        "        # Filter out samples with unknown labels\n",
        "        valid_indices = [i for i, label in enumerate(batch_labels) if label != -1]\n",
        "        batch_images = batch_images[valid_indices]\n",
        "        batch_labels = [batch_labels[i] for i in valid_indices]\n",
        "\n",
        "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "        return batch_images, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/model.h5')\n"
      ],
      "metadata": {
        "id": "uU0ql94nsWcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"your_model_name.h5\")\n"
      ],
      "metadata": {
        "id": "hRmAJfGPsz_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/your_model_name.h5\")\n"
      ],
      "metadata": {
        "id": "M8zDsBNDs41_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content'))\n"
      ],
      "metadata": {
        "id": "jE8pLz4UtCtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('/content/drive/MyDrive'))\n"
      ],
      "metadata": {
        "id": "JJy9FaKBtH7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, data_dir, batch_size, target_size, num_classes):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.num_classes = num_classes\n",
        "        self.image_files = [f for f in os.listdir(data_dir) if f.endswith(\".jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_images = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for file in batch_files:\n",
        "            image_path = os.path.join(self.data_dir, file)\n",
        "            img = tf.keras.preprocessing.image.load_img(image_path, target_size=self.target_size)\n",
        "            img_array = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "            batch_images.append(img_array)\n",
        "\n",
        "            class_label = file.split(\"_\")[0]  # Assuming file names follow a pattern like class_label_img.jpg\n",
        "            label_index = 0 if class_label == 'class_1' else 1\n",
        "            batch_labels.append(label_index)\n",
        "\n",
        "        batch_images = np.array(batch_images)\n",
        "        batch_labels = to_categorical(batch_labels, num_classes=self.num_classes)\n",
        "        return batch_images, batch_labels\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "train_generator = CustomDataGenerator(train_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "validation_generator = CustomDataGenerator(val_dir, batch_size=16, target_size=(150, 150), num_classes=num_classes)\n",
        "test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Save the model correctly this time\n",
        "model.save('/content/model.h5')\n"
      ],
      "metadata": {
        "id": "2joK9W1dtOsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "3gwwLcQ5tUPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive/'))\n"
      ],
      "metadata": {
        "id": "ngBbBrfltdrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('/content/drive/MyDrive/dataset'))\n"
      ],
      "metadata": {
        "id": "6LVNNRyNthAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/My Project/dataset/train\"\n",
        "val_dir = \"/content/drive/MyDrive/My Project/dataset/val\"\n",
        "test_dir = \"/content/drive/MyDrive/My Project/dataset/test\"\n"
      ],
      "metadata": {
        "id": "QvkgxauItliK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir('/content/drive/MyDrive/dataset'))\n"
      ],
      "metadata": {
        "id": "6eSXg-2Ltnll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,  # Adjust if needed\n",
        "    validation_data=validation_generator\n",
        ")\n"
      ],
      "metadata": {
        "id": "k-tZeqKUtuls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),  # Added a dense layer to improve learning\n",
        "    Dense(num_classes, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "93I-mAvvtzD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),  # Define input shape using Input layer\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),  # Added a dense layer to improve learning\n",
        "    Dense(num_classes, activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "gEgNHUlkt29G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator\n",
        ")\n"
      ],
      "metadata": {
        "id": "HMTj4pXVt5qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/dataset/train\"\n",
        "val_dir = \"/content/drive/MyDrive/dataset/val\"\n",
        "test_dir = \"/content/drive/MyDrive/dataset/test\"\n",
        "\n",
        "# Class names\n",
        "class_1_name = \"class_1\"\n",
        "class_2_name = \"class_2\"\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "train_generator = CustomDataGenerator(train_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n",
        "validation_generator = CustomDataGenerator(val_dir, batch_size=16, target_size=(150, 150), num_classes=num_classes)\n",
        "test_generator = CustomDataGenerator(test_dir, batch_size=32, target_size=(150, 150), num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "dsNf8y02t-0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/dataset\"\n",
        "os.listdir(path)\n"
      ],
      "metadata": {
        "id": "yv5NXGVhuDD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "AkcTAHNhuHnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive\"\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "id": "Wz9rYGiPuJiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/content/drive/MyDrive/MARINE DRIVE\"\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "id": "M7GibqF7ukCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/MARINE DRIVE/train\"\n",
        "val_dir = \"/content/drive/MyDrive/MARINE DRIVE/validation\"\n",
        "test_dir = \"/content/drive/MyDrive/MARINE DRIVE/test\"\n"
      ],
      "metadata": {
        "id": "Y3_bWI8Kuq1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Path to the dataset\n",
        "dataset_path = \"/content/drive/MyDrive/MARINE DRIVE\"\n",
        "\n",
        "# Load training dataset\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path + \"/train\",\n",
        "    label_mode=\"int\",  # or 'categorical' for one-hot encoding\n",
        "    image_size=(256, 256),  # Resize all images to 256x256\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Load validation dataset\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path + \"/val\",\n",
        "    label_mode=\"int\",\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Load test dataset\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_path + \"/test\",\n",
        "    label_mode=\"int\",\n",
        "    image_size=(256, 256),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Check dataset shape\n",
        "for images, labels in train_ds.take(1):\n",
        "    print(f\"Image shape: {images.shape}\")\n",
        "    print(f\"Label shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "id": "yVu0wItGur4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/MARINE DRIVE\"\n",
        "print(\"Exists:\", os.path.exists(dataset_path))\n",
        "print(\"Contents:\", os.listdir(dataset_path) if os.path.exists(dataset_path) else \"Path not found\")\n"
      ],
      "metadata": {
        "id": "Qrx2A0aZuykI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import glob\n",
        "\n",
        "# Load .tif files directly\n",
        "file_paths = glob.glob(dataset_path + \"/*.tif\")\n",
        "\n",
        "images = []\n",
        "for file in file_paths:\n",
        "    img = cv2.imread(file, cv2.IMREAD_COLOR)\n",
        "    img = cv2.resize(img, (256, 256))  # Resize to match model input size\n",
        "    images.append(img)\n",
        "\n",
        "images = np.array(images)\n",
        "print(\"Loaded images shape:\", images.shape)\n"
      ],
      "metadata": {
        "id": "KTcdYRslu_a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in file_paths:\n",
        "    img = cv2.imread(file, cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        print(f\"Failed to load: {file}\")  # Debugging line\n",
        "        continue\n",
        "    img = cv2.resize(img, (256, 256))\n",
        "    images.append(img)\n"
      ],
      "metadata": {
        "id": "zyjIi28wvFbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "images = []\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/MARINE DRIVE/S2_3-11-18_16PDC_10_conf.tif\",\n",
        "    \"/content/drive/MyDrive/MARINE DRIVE/S2_3-11-18_16PDC_11_cl.tif\",\n",
        "    # Add other paths here\n",
        "]\n",
        "\n",
        "for file in file_paths:\n",
        "    try:\n",
        "        img = Image.open(file)  # Open with Pillow\n",
        "        img = img.resize((256, 256))  # Resize to match model input size\n",
        "        img = np.array(img)  # Convert to NumPy array\n",
        "        images.append(img)\n",
        "        print(f\"Loaded: {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {file}: {e}\")\n"
      ],
      "metadata": {
        "id": "X_VKqrofvNWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "images = []\n",
        "file_paths = [\n",
        "    \"/content/drive/MyDrive/MARINE DRIVE/S2_3-11-18_16PDC_10_conf.tif\",\n",
        "    \"/content/drive/MyDrive/MARINE DRIVE/S2_3-11-18_16PDC_11_cl.tif\",\n",
        "    # Add other paths here\n",
        "]\n",
        "\n",
        "for file in file_paths:\n",
        "    try:\n",
        "        img = Image.open(file)\n",
        "        img = img.resize((256, 256))  # Resize to match model input size\n",
        "        img = np.array(img)  # Convert to NumPy array\n",
        "        images.append(img)\n",
        "        print(f\"Loaded: {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {file}: {e}\")\n",
        "\n",
        "# Check the shape of the loaded images\n",
        "print(f\"Total images loaded: {len(images)}\")\n",
        "print(f\"Sample image shape: {images[0].shape if images else 'None'}\")\n"
      ],
      "metadata": {
        "id": "BPLtxszZvXmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.array(images) / 255.0\n",
        "print(f\"Final dataset shape: {images.shape}\")\n"
      ],
      "metadata": {
        "id": "0lHM-1GvvecV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.stack(images, axis=0)  # Stack into a single numpy array\n",
        "images = images.reshape((-1, 256, 256, 3))\n",
        "print(f\"Final dataset shape: {images.shape}\")\n"
      ],
      "metadata": {
        "id": "yV7BBSpGvkqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = images.reshape((-1, 256, 256, 1))  # For grayscale images\n"
      ],
      "metadata": {
        "id": "3XU-BW71vnDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0, 1]  # Assuming two classes\n"
      ],
      "metadata": {
        "id": "Zs9PJxvevprx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "print(f\"Train set size: {X_train.shape}, Test set size: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "p3TMHJx1vtDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n"
      ],
      "metadata": {
        "id": "a1hZp6PUvylA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Use 'softmax' for multi-class classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  # Use 'categorical_crossentropy' for multi-class\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "-JA9NNEkvzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "f_GO22YBv13i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.GlobalAveragePooling2D(),  # Reduces dimensions and avoids parameter explosion\n",
        "    layers.Dropout(0.4),  # Helps prevent overfitting\n",
        "    layers.Dense(32, activation='relu'),  # Reduced size to prevent overfitting\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "lEseIO6Mv-Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "6Er3xIgVwBKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Ensure X_train and y_train are numpy arrays of type float32\n",
        "X_train = np.array(X_train, dtype=np.float32)\n",
        "X_test = np.array(X_test, dtype=np.float32)\n",
        "\n",
        "# Ensure y_train and y_test are shaped correctly\n",
        "y_train = np.array(y_train).reshape(-1, 1).astype(np.float32)\n",
        "y_test = np.array(y_test).reshape(-1, 1).astype(np.float32)\n",
        "\n",
        "# Double-check shapes\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=1, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "8DxFTC8PwGGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "pnLzNgBwwNns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "rCuCN9fHwOzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3,3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary output\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=4, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "c-No_5mCwVQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "gklV9GZpwZY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)\n",
        "print(y_test)\n"
      ],
      "metadata": {
        "id": "dxPtECfSweVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Fit the generator to the training data\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Compile with a smaller learning rate\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train using augmented data\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=1),\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n"
      ],
      "metadata": {
        "id": "GXMmXL73wjTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "N_hp6wAbwkpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "CFLgUUSfwwlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "k7_topC5w1uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n"
      ],
      "metadata": {
        "id": "ywJDd8FLw3IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Check shapes\n",
        "print(f\"Train set size: {X_train.shape}, Test set size: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "LlZLqrbMw_zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create Data Augmentation Generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,       # Rotate images by 20 degrees\n",
        "    width_shift_range=0.1,   # Shift horizontally by 10%\n",
        "    height_shift_range=0.1,  # Shift vertically by 10%\n",
        "    shear_range=0.1,         # Shear transformation\n",
        "    zoom_range=0.1,          # Zoom in by 10%\n",
        "    horizontal_flip=True,    # Flip horizontally\n",
        "    fill_mode='nearest'      # Fill missing pixels\n",
        ")\n",
        "\n",
        "# Fit the generator on training data\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Generate augmented data while training\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=1),\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "COXjymd8xGXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "m4eeeCPPxHtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n"
      ],
      "metadata": {
        "id": "mPel9OAUxUXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create Data Augmentation Generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,       # Rotate images by 20 degrees\n",
        "    width_shift_range=0.1,   # Shift horizontally by 10%\n",
        "    height_shift_range=0.1,  # Shift vertically by 10%\n",
        "    shear_range=0.1,         # Shear transformation\n",
        "    zoom_range=0.1,          # Zoom in by 10%\n",
        "    horizontal_flip=True,    # Flip horizontally\n",
        "    fill_mode='nearest'      # Fill missing pixels\n",
        ")\n",
        "\n",
        "# Fit the generator on training data\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Generate augmented data while training\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=1),\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "N3hvK8mXxWYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Retrain the model\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=1),\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "2XrO5xxNxfH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Updated model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Lower learning rate + Early Stopping\n",
        "optimizer = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=1),\n",
        "                    epochs=20,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "e899ABmTxlRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "print(f\"Predictions:\\n{predictions}\")\n"
      ],
      "metadata": {
        "id": "FGYNKXfrxnfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Generate more training data\n",
        "augmented_data = []\n",
        "augmented_labels = []\n",
        "\n",
        "for _ in range(20):  # Generate 20 augmented images per sample\n",
        "    for x, y in datagen.flow(X_train, y_train, batch_size=1):\n",
        "        augmented_data.append(x[0])\n",
        "        augmented_labels.append(y[0])\n",
        "        if len(augmented_data) >= 20 * len(X_train):\n",
        "            break\n",
        "\n",
        "# Convert to numpy array\n",
        "X_train_augmented = np.array(augmented_data)\n",
        "y_train_augmented = np.array(augmented_labels)\n",
        "\n",
        "# Final training set\n",
        "X_train_final = np.concatenate((X_train, X_train_augmented), axis=0)\n",
        "y_train_final = np.concatenate((y_train, y_train_augmented), axis=0)\n",
        "\n",
        "print(f\"Final train set size: {X_train_final.shape}\")\n"
      ],
      "metadata": {
        "id": "uzHeiXqty-YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_final, y_train_final,\n",
        "                    epochs=20,\n",
        "                    batch_size=8,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "Fv12OLBmzBA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Predictions\n",
        "predictions = model.predict(X_test)\n",
        "print(f\"Predictions:\\n{predictions}\")\n"
      ],
      "metadata": {
        "id": "-3EgqW91zEh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained model\n",
        "MODEL_PATH = 'model.h5'  # Adjust the path if needed\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        # Get the image from the request\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({'error': 'No file provided'}), 400\n",
        "\n",
        "        file = request.files['file']\n",
        "        img = Image.open(io.BytesIO(file.read())).convert('L')  # Convert to grayscale\n",
        "\n",
        "        # Resize and normalize the image\n",
        "        img = img.resize((256, 256))\n",
        "        img_array = np.array(img) / 255.0\n",
        "        img_array = img_array.reshape(1, 256, 256, 1)  # Shape to match model input\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(img_array)[0][0]\n",
        "        result = 'Plastic detected' if prediction > 0.5 else 'No plastic detected'\n",
        "\n",
        "        return jsonify({'prediction': float(prediction), 'result': result})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "6uTrmIX3zqM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model.h5')\n"
      ],
      "metadata": {
        "id": "cBa38OTgzwCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "MOYjhWfzzycI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = '/path/to/model.h5'\n"
      ],
      "metadata": {
        "id": "HzV6_ZL6z1Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained model\n",
        "MODEL_PATH = 'model.h5'  # Adjust the path if needed\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        # Get the image from the request\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({'error': 'No file provided'}), 400\n",
        "\n",
        "        file = request.files['file']\n",
        "        img = Image.open(io.BytesIO(file.read())).convert('L')  # Convert to grayscale\n",
        "\n",
        "        # Resize and normalize the image\n",
        "        img = img.resize((256, 256))\n",
        "        img_array = np.array(img) / 255.0\n",
        "        img_array = img_array.reshape(1, 256, 256, 1)  # Shape to match model input\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(img_array)[0][0]\n",
        "        result = 'Plastic detected' if prediction > 0.5 else 'No plastic detected'\n",
        "\n",
        "        return jsonify({'prediction': float(prediction), 'result': result})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "0n5w_zWbz4W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "\n",
        "# Load and encode image\n",
        "with open('path_to_test_image.jpg', 'rb') as f:\n",
        "    img_data = base64.b64encode(f.read()).decode('utf-8')\n",
        "\n",
        "# Send request\n",
        "url = 'http://127.0.0.1:5000/predict'\n",
        "response = requests.post(url, json={'image': img_data})\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "Dnx0YmmN0DQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Create a random image (256x256)\n",
        "img = np.random.randint(0, 256, (256, 256, 3), dtype=np.uint8)\n",
        "img = Image.fromarray(img)\n",
        "img.save('/content/sample_image.jpg')\n"
      ],
      "metadata": {
        "id": "SD3dvf1j0mYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "\n",
        "# Load and encode image\n",
        "with open('/content/sample_image.jpg', 'rb') as f:\n",
        "    img_data = base64.b64encode(f.read()).decode('utf-8')\n",
        "\n",
        "# Send POST request to the Flask server\n",
        "url = 'http://127.0.0.1:5000/predict'\n",
        "data = {'image': img_data}\n",
        "response = requests.post(url, json=data)\n",
        "\n",
        "# Print the response\n",
        "print('Response:', response.json())\n"
      ],
      "metadata": {
        "id": "av9kk64j0srx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n"
      ],
      "metadata": {
        "id": "zANooP-n0xS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data Augmentation to increase dataset size\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.2\n",
        ")\n",
        "\n",
        "# Load your existing dataset\n",
        "train_images = np.random.rand(100, 256, 256, 1)  # Example size (increase dataset size)\n",
        "train_labels = np.random.randint(0, 2, size=(100, 1))\n",
        "test_images = np.random.rand(20, 256, 256, 1)  # Example size\n",
        "test_labels = np.random.randint(0, 2, size=(20, 1))\n",
        "\n",
        "# Model Architecture (Deeper + Regularization)\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),  # Dropout to prevent overfitting\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model (Lower learning rate)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model with data augmentation\n",
        "train_gen = datagen.flow(train_images, train_labels, batch_size=16)\n",
        "\n",
        "history = model.fit(train_gen, epochs=20, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Save model\n",
        "model.save('model.h5')\n"
      ],
      "metadata": {
        "id": "iMVSvJUI1Gqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Load and preprocess image\n",
        "img = cv2.imread('path_to_test_image.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "img = cv2.resize(img, (256, 256))\n",
        "img = img.reshape(1, 256, 256, 1) / 255.0\n",
        "\n",
        "# Prediction\n",
        "prediction = model.predict(img)[0][0]\n",
        "if prediction > 0.5:\n",
        "    print(\"Plastic waste detected!\")\n",
        "else:\n",
        "    print(\"No plastic waste detected.\")\n"
      ],
      "metadata": {
        "id": "lI1PdWxy1QMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('/content/path_to_test_image.jpg', cv2.IMREAD_GRAYSCALE)\n"
      ],
      "metadata": {
        "id": "v01yLD2u3qrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('path_to_test_image.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "if img is None:\n",
        "    print(\"Error: Image not found or could not be loaded.\")\n",
        "else:\n",
        "    img = cv2.resize(img, (256, 256))\n"
      ],
      "metadata": {
        "id": "KhvZuhNe3tD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists('/content/path_to_test_image.jpg'))\n"
      ],
      "metadata": {
        "id": "Ekd-myF53ynt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bing-image-downloader\n"
      ],
      "metadata": {
        "id": "0WILHnwx4JS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bing_image_downloader import downloader\n",
        "\n",
        "# Create a folder and download images\n",
        "downloader.download(\"ocean plastic waste\", limit=5, output_dir='images', adult_filter_off=True, force_replace=False, timeout=60)\n"
      ],
      "metadata": {
        "id": "hb0dncfg4LEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('images/ocean plastic waste'))\n"
      ],
      "metadata": {
        "id": "QdXbM_Xb4J_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the first image\n",
        "img = cv2.imread('images/ocean plastic waste/Image_1.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "if img is not None:\n",
        "    img = cv2.resize(img, (256, 256))\n",
        "    img = img.reshape(1, 256, 256, 1) / 255.0\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = model.predict(img)\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "else:\n",
        "    print(\"Error: Image not found or could not be loaded.\")\n"
      ],
      "metadata": {
        "id": "0jdrGTH94TJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder = 'images/ocean plastic waste'\n",
        "images = os.listdir(folder)\n",
        "\n",
        "for img_name in images:\n",
        "    img_path = os.path.join(folder, img_name)\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if img is not None:\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        img = img.reshape(1, 256, 256, 1) / 255.0\n",
        "\n",
        "        prediction = model.predict(img)\n",
        "        print(f\"{img_name} Prediction: {prediction}\")\n",
        "    else:\n",
        "        print(f\"Error loading {img_name}\")\n"
      ],
      "metadata": {
        "id": "CZr9P_644UoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load and augment images\n",
        "train_data = datagen.flow_from_directory(\n",
        "    'path_to_training_data',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Build improved model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_data, epochs=20)\n"
      ],
      "metadata": {
        "id": "7YeJyWAy4Ydt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create folders\n",
        "os.makedirs('/content/training_data/plastic', exist_ok=True)\n",
        "os.makedirs('/content/training_data/non_plastic', exist_ok=True)\n",
        "\n",
        "# Move downloaded files to the right place\n",
        "downloaded_files = ['Image_1.jpg', 'Image_2.jpeg', 'Image_3.jpg', 'Image_4.jpg', 'Image_5.jpg']\n",
        "\n",
        "for i, file in enumerate(downloaded_files):\n",
        "    if i % 2 == 0:  # Alternate between plastic and non-plastic for now\n",
        "        shutil.move(file, '/content/training_data/plastic/')\n",
        "    else:\n",
        "        shutil.move(file, '/content/training_data/non_plastic/')\n"
      ],
      "metadata": {
        "id": "skR5KjP54g4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List files in the current directory\n",
        "print(os.listdir())\n"
      ],
      "metadata": {
        "id": "BGo_GenE4sNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "urls = [\n",
        "    'https://source.unsplash.com/400x400/?plastic',\n",
        "    'https://source.unsplash.com/400x400/?ocean',\n",
        "    'https://source.unsplash.com/400x400/?waste',\n",
        "    'https://source.unsplash.com/400x400/?trash',\n",
        "    'https://source.unsplash.com/400x400/?pollution'\n",
        "]\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(f'Image_{i + 1}.jpg', 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f'Downloaded Image_{i + 1}.jpg')\n",
        "    else:\n",
        "        print(f'Failed to download Image_{i + 1}')\n"
      ],
      "metadata": {
        "id": "foFKjSO_4yez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "# New image URLs (replace if needed)\n",
        "image_urls = [\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/3/3f/Floating_plastic_waste.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/0/0a/Plastic_pollution_on_beach.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/5/5e/Ocean_plastic_pollution.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/8/88/Plastic_pollution_in_water.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/9/96/Plastic_waste_in_ocean.jpg'\n",
        "]\n",
        "\n",
        "for i, url in enumerate(image_urls):\n",
        "    try:\n",
        "        file_path = f'images/Image_{i+1}.jpg'\n",
        "        urllib.request.urlretrieve(url, file_path)\n",
        "        print(f\"Downloaded: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download Image_{i+1}: {e}\")\n",
        "\n",
        "print(\"\\n✅ Download Complete!\")\n"
      ],
      "metadata": {
        "id": "cRLES-y_40Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ✅ Ensure training data path is correct\n",
        "TRAINING_DATA_DIR = '/content/training_data'\n",
        "if not os.path.exists(TRAINING_DATA_DIR):\n",
        "    print(f\"Training data directory '{TRAINING_DATA_DIR}' not found.\")\n",
        "else:\n",
        "    print(f\"Training data directory found at: {TRAINING_DATA_DIR}\")\n",
        "\n",
        "# ✅ Data Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # 80% train, 20% validation\n",
        ")\n",
        "\n",
        "# ✅ Load Training Data\n",
        "train_data = datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# ✅ Model Definition\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification (plastic vs non-plastic)\n",
        "])\n",
        "\n",
        "# ✅ Compile Model\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ✅ Train Model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    validation_data=val_data\n",
        ")\n",
        "\n",
        "# ✅ Evaluate Model\n",
        "loss, accuracy = model.evaluate(val_data)\n",
        "print(f'\\nValidation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# ✅ Plot Accuracy and Loss Curves\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Model Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Model Loss')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# ✅ Confusion Matrix\n",
        "y_true = val_data.classes\n",
        "y_pred = (model.predict(val_data) > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=train_data.class_indices, yticklabels=train_data.class_indices)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# ✅ Save Model\n",
        "model.save('waste_detection_model.h5')\n",
        "print(\"\\n✅ Model saved as 'waste_detection_model.h5'\")\n"
      ],
      "metadata": {
        "id": "GzWBRfk9G9my"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "id": "2h7hPZ9UHhEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('/content/training_data/plastic', exist_ok=True)\n",
        "\n",
        "# Move images to training folder\n",
        "image_files = ['/content/images/Image_1.jpg', '/content/images/Image_2.jpeg',\n",
        "               '/content/images/Image_3.jpg', '/content/images/Image_4.jpg',\n",
        "               '/content/images/Image_5.jpg']\n",
        "\n",
        "for img in image_files:\n",
        "    if os.path.exists(img):\n",
        "        shutil.move(img, '/content/training_data/plastic/')\n",
        "    else:\n",
        "        print(f\"❌ {img} not found\")\n",
        "\n",
        "print(\"✅ Files moved to training_data/plastic/\")\n"
      ],
      "metadata": {
        "id": "vE9107nUHkqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('/content/training_data/plastic', exist_ok=True)\n",
        "\n",
        "# Image URLs (sample ocean plastic images)\n",
        "urls = [\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/7/72/Plastic_debris_on_beach.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/6/65/Marine_debris_in_Hawaii.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/4/44/Beach_litter_%2822237610505%29.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/3/3b/Plastic_bottles_beach.jpg',\n",
        "    'https://upload.wikimedia.org/wikipedia/commons/8/82/Ocean_plastic_pollution.jpg'\n",
        "]\n",
        "\n",
        "# Download images\n",
        "for i, url in enumerate(urls):\n",
        "    file_path = f'/content/training_data/plastic/Image_{i+1}.jpg'\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"✅ Downloaded: {file_path}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed to download Image_{i+1}: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading Image_{i+1}: {e}\")\n",
        "\n",
        "print(\"✅ All available images downloaded!\")\n"
      ],
      "metadata": {
        "id": "q-w1ftTeHrEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('/content/training_data/plastic', exist_ok=True)\n",
        "\n",
        "# New image URLs (open-source marine debris images)\n",
        "urls = [\n",
        "    'https://images.unsplash.com/photo-1567443029981-45c8f7690b44',\n",
        "    'https://images.unsplash.com/photo-1580653680274-761389397817',\n",
        "    'https://images.unsplash.com/photo-1613339081872-46d154ff0348',\n",
        "    'https://images.unsplash.com/photo-1612224035354-5b8bb63ff215',\n",
        "    'https://images.unsplash.com/photo-1603133872872-bf4c4a24334a'\n",
        "]\n",
        "\n",
        "# Download images\n",
        "for i, url in enumerate(urls):\n",
        "    file_path = f'/content/training_data/plastic/Image_{i+1}.jpg'\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            with open(file_path, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"✅ Downloaded: {file_path}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed to download Image_{i+1}: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading Image_{i+1}: {e}\")\n",
        "\n",
        "print(\"✅ All available images downloaded!\")\n"
      ],
      "metadata": {
        "id": "U6lnKq-4Hx3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Create directory for images\n",
        "os.makedirs('/content/training_data/plastic', exist_ok=True)\n",
        "\n",
        "# NOAA Marine Debris Photo Gallery URL\n",
        "base_url = 'https://marinedebris.noaa.gov/multimedia/photos'\n",
        "\n",
        "# Fetch the gallery page\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image links\n",
        "image_tags = soup.find_all('img')\n",
        "\n",
        "# Download images\n",
        "for i, img_tag in enumerate(image_tags):\n",
        "    img_url = img_tag.get('src')\n",
        "    if img_url and img_url.startswith('/'):\n",
        "        img_url = 'https://marinedebris.noaa.gov' + img_url\n",
        "    elif not img_url.startswith('http'):\n",
        "        continue\n",
        "\n",
        "    file_path = f'/content/training_data/plastic/Image_{i+1}.jpg'\n",
        "    try:\n",
        "        img_data = requests.get(img_url).content\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(img_data)\n",
        "        print(f\"✅ Downloaded: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {img_url}: {e}\")\n",
        "\n",
        "print(\"✅ All available images downloaded!\")\n"
      ],
      "metadata": {
        "id": "BikBV2e5H9vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define training data path\n",
        "TRAINING_DATA_DIR = '/content/training_data'\n",
        "\n",
        "# Create an ImageDataGenerator for augmentation and normalization\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,          # Normalize pixel values\n",
        "    rotation_range=20,       # Rotate images up to 20 degrees\n",
        "    width_shift_range=0.2,   # Shift width by 20%\n",
        "    height_shift_range=0.2,  # Shift height by 20%\n",
        "    shear_range=0.2,         # Shear transformations\n",
        "    zoom_range=0.2,          # Zoom in/out\n",
        "    horizontal_flip=True,    # Flip images horizontally\n",
        "    fill_mode='nearest'      # Fill in new pixels\n",
        ")\n",
        "\n",
        "# Load training data\n",
        "train_data = datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR,\n",
        "    target_size=(256, 256),     # Resize all images to 256x256\n",
        "    batch_size=32,              # Batch size of 32\n",
        "    class_mode='categorical'     # Multiple classes (if applicable)\n",
        ")\n",
        "\n",
        "# ✅ Check the class labels\n",
        "print(f\"\\nClass Labels: {train_data.class_indices}\")\n"
      ],
      "metadata": {
        "id": "8fvhuug8ILOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# ✅ Build the model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(256, 256, 3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),  # Dropout for regularization\n",
        "    Dense(1, activation='sigmoid')  # Binary classification (plastic or not)\n",
        "])\n",
        "\n",
        "# ✅ Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ✅ Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ✅ Save the model\n",
        "model.save('/content/model.h5')\n",
        "\n",
        "# ✅ Evaluate the model\n",
        "loss, accuracy = model.evaluate(train_data)\n",
        "print(f\"\\n✅ Model Accuracy: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "TPX-T95RITYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# ✅ Data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    '/content/training_data',\n",
        "    target_size=(256, 256),\n",
        "    batch_size=8,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# ✅ Build model with regularization\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.001), input_shape=(256, 256, 3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    Dropout(0.5),  # Dropout for regularization\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# ✅ Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# ✅ Train model with augmented data\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ✅ Save in new Keras format\n",
        "model.save('/content/model.keras')\n",
        "\n",
        "# ✅ Evaluate\n",
        "loss, accuracy = model.evaluate(train_data)\n",
        "print(f\"\\n✅ Model Accuracy: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "3RKRaiwcIsWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    brightness_range=[0.8, 1.2],  # New: Brightness variation\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,  # New: Vertical flip\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', kernel_regularizer=l2(0.002), input_shape=(256, 256, 3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "    Dropout(0.6),  # Increased dropout rate\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(train_data),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.save('/content/model.keras')\n",
        "\n",
        "loss, accuracy = model.evaluate(train_data)\n",
        "print(f\"\\n✅ Model Accuracy: {accuracy:.2%}\")\n"
      ],
      "metadata": {
        "id": "428k4UMoIwWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask flask-cors\n"
      ],
      "metadata": {
        "id": "V4gXLd_zKdRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "p4lvfgEhKiQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=False)\n"
      ],
      "metadata": {
        "id": "UtM6Qn9FK5Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "id": "Oq1_tNe1LCHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/uploads\n"
      ],
      "metadata": {
        "id": "ZDXqBfCaLbXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!readlink -f /content/uploads/your_image_file_name.jpg\n"
      ],
      "metadata": {
        "id": "AC65c_8vLdJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/uploads\n"
      ],
      "metadata": {
        "id": "-DNMHbZzLh0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /mnt/data/image.png /content/uploads/\n"
      ],
      "metadata": {
        "id": "KSI5BvJYLwjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /mnt/data/image.png /content/uploads/\n"
      ],
      "metadata": {
        "id": "4ATZ9_SmL15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'http://127.0.0.1:5000/predict'\n",
        "files = {'file': open('/content/training_data/plastic/Image_1.jpg', 'rb')}\n",
        "response = requests.post(url, files=files)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "YqPtvVWIMrDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "id": "-LAV-GrBMwvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Flask server on port 5000\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"🌐 Public URL: {public_url}\")\n"
      ],
      "metadata": {
        "id": "9_RY_3-DMx4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "!ngrok authtoken 2ueqHxy9ecmZqut8ry1lk5tWHDs_78d8j6TzjAi2rMe9nqQrS\n"
      ],
      "metadata": {
        "id": "1egRBToANsUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Flask server on port 5000\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"🌐 Public URL: {public_url}\")\n"
      ],
      "metadata": {
        "id": "GjcR_p4mNv8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/model.h5')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        file = request.files['file']\n",
        "        if not file:\n",
        "            return jsonify({\"error\": \"No file uploaded\"}), 400\n",
        "\n",
        "        # Read the image\n",
        "        img = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (224, 224))  # Resize to model input size\n",
        "        img = img / 255.0  # Normalize the image\n",
        "        img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(img)\n",
        "        result = \"Plastic\" if prediction[0][0] > 0.5 else \"Non-Plastic\"\n",
        "\n",
        "        return jsonify({\"prediction\": result, \"confidence\": float(prediction[0][0])})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "tL179AQnN5uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://9d90-104-198-77-180.ngrok-free.app/predict'\n",
        "files = {'file': open('/content/training_data/plastic/Image_1.jpg', 'rb')}\n",
        "response = requests.post(url, files=files)\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "ANkm4u9zOCTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        print(f\"Request: {request}\")  # Log the request\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({'error': 'No file part'}), 400\n",
        "\n",
        "        file = request.files['file']\n",
        "        if file.filename == '':\n",
        "            return jsonify({'error': 'No selected file'}), 400\n",
        "\n",
        "        # Save and read the file\n",
        "        file_path = os.path.join('/content/uploads', file.filename)\n",
        "        file.save(file_path)\n",
        "\n",
        "        # Preprocess the image\n",
        "        img = image.load_img(file_path, target_size=(150, 150))\n",
        "        img_array = image.img_to_array(img)\n",
        "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "        # Prediction\n",
        "        prediction = model.predict(img_array)\n",
        "        predicted_class = \"Plastic\" if prediction[0][0] > 0.5 else \"Non-Plastic\"\n",
        "        confidence = float(prediction[0][0])\n",
        "\n",
        "        # Return the response\n",
        "        response = jsonify({\n",
        "            'prediction': predicted_class,\n",
        "            'confidence': confidence\n",
        "        })\n",
        "        print(f\"Response: {response.get_json()}\")  # Log the response\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")  # Log the error\n",
        "        return jsonify({'error': str(e)}), 500\n"
      ],
      "metadata": {
        "id": "agzwcoBnOEs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os._exit(00)\n"
      ],
      "metadata": {
        "id": "dv-3Yj71OYKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir('/content/'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDjhKJuWP6LL",
        "outputId": "294ec60f-209d-4f82-d7a3-0f1efaa2d791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'model.keras', 'model.h5', 'uploads', 'training_data', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doy0YzQzQJI0",
        "outputId": "7b2be8b6-08c4-4a92-e14e-02fce9402251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/model.keras')\n"
      ],
      "metadata": {
        "id": "OUuPgb1gQI2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "TZmW8NBhQUk9",
        "outputId": "0f3b54c1-4124-4fa5-ff81-278b056eaba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m115200\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │      \u001b[38;5;34m29,491,456\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m257\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">115200</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">29,491,456</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m88,754,885\u001b[0m (338.57 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">88,754,885</span> (338.57 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m29,584,961\u001b[0m (112.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">29,584,961</span> (112.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m59,169,924\u001b[0m (225.72 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">59,169,924</span> (225.72 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.input_shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHBIAE71Qfjf",
        "outputId": "7eb1e997-964a-4cf0-c735-497b3b79e98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 256, 256, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = tf.image.resize(sample_input, (256, 256))\n"
      ],
      "metadata": {
        "id": "p0LREfdbQgfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Resize the sample input to match the expected shape\n",
        "sample_input = np.random.random((1, 224, 224, 3))\n",
        "sample_input = tf.image.resize(sample_input, (256, 256))\n",
        "\n",
        "# Ensure input shape matches the model's expected shape\n",
        "print(\"Sample input shape:\", sample_input.shape)\n",
        "\n",
        "# Predict using the model\n",
        "prediction = model.predict(sample_input)\n",
        "\n",
        "# Output the prediction\n",
        "print(\"Prediction:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmc2LfV2QqtU",
        "outputId": "fdbec7f3-93ed-4e23-b27e-2758c07cf16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input shape: (1, 256, 256, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 628ms/step\n",
            "Prediction: [[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn python-multipart\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMqLJPbiQ1tD",
        "outputId": "68f40260-5235-4a7e-d4dd-2e06ef3ed13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, python-multipart, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.11 python-multipart-0.0.20 starlette-0.46.1 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, UploadFile, File\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import io\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Define the input shape expected by the model\n",
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        # Read the uploaded image\n",
        "        contents = await file.read()\n",
        "        image = Image.open(io.BytesIO(contents)).convert('RGB')\n",
        "        image = image.resize(IMG_SIZE)\n",
        "        image = np.array(image) / 255.0  # Normalize the image\n",
        "        image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "\n",
        "        # Make a prediction\n",
        "        prediction = model.predict(image)\n",
        "        result = float(prediction[0][0])  # Adjust based on output format\n",
        "\n",
        "        return {\"prediction\": result}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Run the server (use in local environment)\n",
        "# uvicorn main:app --host 0.0.0.0 --port 8000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsZCKsIHQ4c7",
        "outputId": "3db1b07e-4e24-4496-da05-07d14d395cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# Compile the model (optional, but helps to avoid the warning)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fybi5dYRB55",
        "outputId": "61552060-5174-4ed0-8a2f-00d56a5302eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import absl.logging\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        if not data or 'input' not in data:\n",
        "            return jsonify({'error': 'Invalid input data'}), 400\n",
        "\n",
        "        # Convert input data to a numpy array and reshape to match model input shape\n",
        "        input_data = np.array(data['input']).reshape(1, 256, 256, 3)\n",
        "        prediction = model.predict(input_data)\n",
        "\n",
        "        # Convert prediction to a list (if it's not already)\n",
        "        prediction_list = prediction.tolist()\n",
        "\n",
        "        return jsonify({'prediction': prediction_list})\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydjsfDC-RNKZ",
        "outputId": "a4f31e66-dacc-4df9-f24b-a55700bd788a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep flask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sj3rkacR6-i",
        "outputId": "af8ecf8c-c8ba-483e-ee49-26f7ee9fa15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       12658  0.0  0.0   7376  3520 ?        S    05:16   0:00 /bin/bash -c ps aux | grep flask\n",
            "root       12660  0.0  0.0   6484  2296 ?        S    05:16   0:00 grep flask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnM3pbZcSAht",
        "outputId": "2fd50177-b932-4c86-dec9-335142684007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a public URL for Flask\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Start Flask app\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxPc_JqWSB-4",
        "outputId": "a02c56a3-dc4e-42d7-f30c-eba08e466211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://8d87-104-198-77-180.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Open a public URL for Flask\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Start Flask app\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STmTBLarSMph",
        "outputId": "d5c75d55-bf09-48f5-b0c7-dd5c222c5acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://9a21-104-198-77-180.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://your-ngrok-url.ngrok-free.app/predict'\n",
        "data = {\"input\": [[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]]}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "8EqCEAV0SO-y",
        "outputId": "fb610f7e-dd9d-4e86-bb28-36631231de51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, allow_nan, **kw)\u001b[0m\n\u001b[1;32m    513\u001b[0m             and not use_decimal and not allow_nan and not kw):\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-89ac8b6b4cc9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2ueqHxy9ecmZqut8ry1lk5tWHDs_78d8j6TzjAi2rMe9nqQrS\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ue5i8aRSU3a",
        "outputId": "7e111041-dda3-4dbb-963a-22a510c27aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from flask import Flask, request, jsonify\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model('/content/model.h5')\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        input_data = np.array(data['input'])\n",
        "        prediction = model.predict(input_data)\n",
        "        return jsonify({'prediction': prediction.tolist()})\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)})\n",
        "\n",
        "# Start ngrok tunnel and Flask app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnQi946MScxi",
        "outputId": "263ff0ef-3a7d-49af-b583-ffbffd897adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://0286-104-198-77-180.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://your-ngrok-url.ngrok-free.app/predict'  # Replace with your ngrok URL\n",
        "\n",
        "data = {\n",
        "    \"input\": np.random.random((1, 256, 256, 3)).tolist()\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "LpRAnBR6Sfqq",
        "outputId": "7157dfe8-82af-46ca-af55-1016279ed1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SSLError",
          "evalue": "HTTPSConnectionPool(host='your-ngrok-url.ngrok-free.app', port=443): Max retries exceeded with url: /predict (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2427)')))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;31mSSLError\u001b[0m: EOF occurred in violation of protocol (_ssl.c:2427)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='your-ngrok-url.ngrok-free.app', port=443): Max retries exceeded with url: /predict (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2427)')))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-152455123faa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0;31m# This branch is for urllib3 v1.22 and later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSSLError\u001b[0m: HTTPSConnectionPool(host='your-ngrok-url.ngrok-free.app', port=443): Max retries exceeded with url: /predict (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2427)')))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Create directory for images\n",
        "os.makedirs('/content/training_data/plastic', exist_ok=True)\n",
        "\n",
        "# NOAA Marine Debris Photo Gallery URL\n",
        "base_url = 'https://marinedebris.noaa.gov/multimedia/photos'\n",
        "\n",
        "# Fetch the gallery page\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all image links\n",
        "image_tags = soup.find_all('img')\n",
        "\n",
        "# Download images\n",
        "for i, img_tag in enumerate(image_tags):\n",
        "    img_url = img_tag.get('src')\n",
        "    if img_url and img_url.startswith('/'):\n",
        "        img_url = 'https://marinedebris.noaa.gov' + img_url\n",
        "    elif not img_url.startswith('http'):\n",
        "        continue\n",
        "\n",
        "    file_path = f'/content/training_data/plastic/Image_{i+1}.jpg'\n",
        "    try:\n",
        "        img_data = requests.get(img_url).content\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(img_data)\n",
        "        print(f\"✅ Downloaded: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {img_url}: {e}\")\n",
        "\n",
        "print(\"✅ All available images downloaded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlzIN1MCTfLS",
        "outputId": "61cfdee3-dfe1-4d10-de26-216fb782a9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Downloaded: /content/training_data/plastic/Image_1.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_2.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_3.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_4.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_5.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_6.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_7.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_8.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_9.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_10.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_11.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_12.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_13.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_14.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_15.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_16.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_17.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_18.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_19.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_20.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_21.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_22.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_23.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_24.jpg\n",
            "✅ Downloaded: /content/training_data/plastic/Image_25.jpg\n",
            "✅ All available images downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def check_model():\n",
        "    try:\n",
        "        model = tf.keras.models.load_model('model.keras')\n",
        "        print(\"✅ Model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Model not found or error in loading: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n🔍 **Project Status Check:**\\n\")\n",
        "    check_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B64A5ikhUToA",
        "outputId": "5e21a59f-21a9-4abb-86cd-14633e3e7d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 **Project Status Check:**\n",
            "\n",
            "✅ Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alizahh-7/AI-Waste-Detection.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhPLq8fU1R5J",
        "outputId": "11887750-7fc8-44e9-f875-8d51dab1dc29"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AI-Waste-Detection'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}